{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f5ec82",
   "metadata": {},
   "source": [
    "# Cuffless Blood Pressure - Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis for the cuffless blood pressure estimation project. The analysis covers:\n",
    "\n",
    "## üìä EDA Structure\n",
    "\n",
    "1. **Data Loading & Overview** - Dataset statistics and basic information\n",
    "2. **Signal Analysis** - PPG/ECG signal characteristics and quality assessment  \n",
    "3. **Blood Pressure Distribution** - Systolic and diastolic BP patterns\n",
    "4. **Demographics Analysis** - Age, gender, height, weight distributions and correlations\n",
    "5. **Signal Quality Assessment** - Noise detection, signal-to-noise ratio analysis\n",
    "6. **Feature Relationships** - Correlations between demographics and BP values\n",
    "7. **Signal Visualization** - Time-domain and frequency-domain analysis\n",
    "8. **Data Quality Checks** - Missing values, outliers, and anomaly detection\n",
    "9. **Statistical Analysis** - Hypothesis testing and significance analysis\n",
    "10. **Preprocessing Insights** - Signal characteristics for model preparation\n",
    "\n",
    "## üéØ Objective\n",
    "Understand the PulseDB dataset characteristics to inform preprocessing decisions and model architecture choices for accurate blood pressure estimation from physiological signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once only\n",
    "# %pip install --upgrade pip\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb73a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the processed data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for processed data\n",
    "processed_dir = Path('../data/processed')\n",
    "npy_files = list(processed_dir.glob('*.npy'))\n",
    "\n",
    "if npy_files:\n",
    "    print(f\"üìä Found {len(npy_files)} processed data files:\")\n",
    "    for file in npy_files:\n",
    "        print(f\"   - {file.name}\")\n",
    "    \n",
    "    # Load the data\n",
    "    try:\n",
    "        signals_sbp = np.load(processed_dir / 'signals_sbp.npy')\n",
    "        sbp_labels = np.load(processed_dir / 'sbp_labels.npy')\n",
    "        demographics_sbp = np.load(processed_dir / 'demographics_sbp.npy')\n",
    "        \n",
    "        print(f\"\\nüìà Dataset Statistics:\")\n",
    "        print(f\"Number of samples: {len(signals_sbp)}\")\n",
    "        print(f\"Signal length: {signals_sbp.shape[1]}\")\n",
    "        print(f\"Demographics features: {demographics_sbp.shape[1]} (Age, Gender, Height, Weight)\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Processed .npy files not found. Please run the preprocessing notebook first.\")\n",
    "        signals_sbp = sbp_labels = demographics_sbp = None\n",
    "else:\n",
    "    print(\"‚ùå No processed data found. Please run the preprocessing notebook first.\")\n",
    "    signals_sbp = sbp_labels = demographics_sbp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead41ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DATASET OVERVIEW & BASIC STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None and sbp_labels is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìà DATASET OVERVIEW\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Dataset size: {len(signals_sbp):,} samples\")\n",
    "    print(f\"Signal length: {signals_sbp.shape[1]:,} data points\")\n",
    "    print(f\"Signal duration: ~{signals_sbp.shape[1]/125:.1f} seconds (assuming 125 Hz)\")\n",
    "    print(f\"Demographics features: {demographics_sbp.shape[1]} (Age, Gender, Height, Weight)\")\n",
    "    \n",
    "    # Memory usage\n",
    "    signal_size_mb = signals_sbp.nbytes / (1024**2)\n",
    "    total_size_mb = (signals_sbp.nbytes + sbp_labels.nbytes + demographics_sbp.nbytes) / (1024**2)\n",
    "    print(f\"Memory usage: {total_size_mb:.1f} MB (signals: {signal_size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üî¢ BLOOD PRESSURE STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # SBP statistics\n",
    "    print(\"Systolic Blood Pressure (SBP):\")\n",
    "    print(f\"  Mean: {np.mean(sbp_labels):.1f} ¬± {np.std(sbp_labels):.1f} mmHg\")\n",
    "    print(f\"  Range: {np.min(sbp_labels):.1f} - {np.max(sbp_labels):.1f} mmHg\")\n",
    "    print(f\"  Median: {np.median(sbp_labels):.1f} mmHg\")\n",
    "    print(f\"  25th percentile: {np.percentile(sbp_labels, 25):.1f} mmHg\")\n",
    "    print(f\"  75th percentile: {np.percentile(sbp_labels, 75):.1f} mmHg\")\n",
    "    \n",
    "    # BP categories (American Heart Association guidelines)\n",
    "    normal = np.sum(sbp_labels < 120)\n",
    "    elevated = np.sum((sbp_labels >= 120) & (sbp_labels < 130))\n",
    "    stage1 = np.sum((sbp_labels >= 130) & (sbp_labels < 140))\n",
    "    stage2 = np.sum(sbp_labels >= 140)\n",
    "    \n",
    "    print(\"\\nüìä BP Categories (AHA Guidelines):\")\n",
    "    print(f\"  Normal (<120): {normal:,} ({100*normal/len(sbp_labels):.1f}%)\")\n",
    "    print(f\"  Elevated (120-129): {elevated:,} ({100*elevated/len(sbp_labels):.1f}%)\")\n",
    "    print(f\"  Stage 1 (130-139): {stage1:,} ({100*stage1/len(sbp_labels):.1f}%)\")\n",
    "    print(f\"  Stage 2 (‚â•140): {stage2:,} ({100*stage2/len(sbp_labels):.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üë• DEMOGRAPHICS OVERVIEW\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Demographics statistics\n",
    "    age, gender, height, weight = demographics_sbp.T\n",
    "    \n",
    "    print(\"Age:\")\n",
    "    print(f\"  Mean: {np.mean(age):.1f} ¬± {np.std(age):.1f} years\")\n",
    "    print(f\"  Range: {np.min(age):.0f} - {np.max(age):.0f} years\")\n",
    "    \n",
    "    print(\"\\nGender Distribution:\")\n",
    "    male_count = np.sum(gender == 1)\n",
    "    female_count = np.sum(gender == 0)\n",
    "    print(f\"  Male: {male_count:,} ({100*male_count/len(gender):.1f}%)\")\n",
    "    print(f\"  Female: {female_count:,} ({100*female_count/len(gender):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nHeight: {np.mean(height):.1f} ¬± {np.std(height):.1f} cm\")\n",
    "    print(f\"Weight: {np.mean(weight):.1f} ¬± {np.std(weight):.1f} kg\")\n",
    "    \n",
    "    # BMI calculation\n",
    "    height_m = height / 100  # convert cm to meters\n",
    "    bmi = weight / (height_m ** 2)\n",
    "    print(f\"BMI: {np.mean(bmi):.1f} ¬± {np.std(bmi):.1f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded. Please run preprocessing first.\")\n",
    "    print(\"This section will show comprehensive dataset statistics when data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d368d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. BLOOD PRESSURE DISTRIBUTION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None and sbp_labels is not None:\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Blood Pressure Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Histogram of SBP values\n",
    "    axes[0, 0].hist(sbp_labels, bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(np.mean(sbp_labels), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(sbp_labels):.1f} mmHg')\n",
    "    axes[0, 0].axvline(np.median(sbp_labels), color='orange', linestyle='--', \n",
    "                       label=f'Median: {np.median(sbp_labels):.1f} mmHg')\n",
    "    axes[0, 0].set_xlabel('Systolic Blood Pressure (mmHg)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('SBP Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot of SBP\n",
    "    bp = axes[0, 1].boxplot(sbp_labels, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    axes[0, 1].set_ylabel('Systolic Blood Pressure (mmHg)')\n",
    "    axes[0, 1].set_title('SBP Box Plot')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add outlier statistics\n",
    "    Q1 = np.percentile(sbp_labels, 25)\n",
    "    Q3 = np.percentile(sbp_labels, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = np.sum((sbp_labels < lower_bound) | (sbp_labels > upper_bound))\n",
    "    axes[0, 1].text(0.7, 0.95, f'Outliers: {outliers} ({100*outliers/len(sbp_labels):.1f}%)', \n",
    "                    transform=axes[0, 1].transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "    \n",
    "    # BP categories pie chart\n",
    "    bp_categories = ['Normal (<120)', 'Elevated (120-129)', 'Stage 1 (130-139)', 'Stage 2 (‚â•140)']\n",
    "    bp_counts = [\n",
    "        np.sum(sbp_labels < 120),\n",
    "        np.sum((sbp_labels >= 120) & (sbp_labels < 130)),\n",
    "        np.sum((sbp_labels >= 130) & (sbp_labels < 140)),\n",
    "        np.sum(sbp_labels >= 140)\n",
    "    ]\n",
    "    colors = ['lightgreen', 'yellow', 'orange', 'red']\n",
    "    \n",
    "    wedges, texts, autotexts = axes[1, 0].pie(bp_counts, labels=bp_categories, autopct='%1.1f%%', \n",
    "                                              colors=colors, startangle=90)\n",
    "    axes[1, 0].set_title('BP Categories Distribution (AHA Guidelines)')\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    from scipy import stats\n",
    "    stats.probplot(sbp_labels, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot: SBP Normality Check')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä STATISTICAL TESTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Normality test\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(sbp_labels[:5000] if len(sbp_labels) > 5000 else sbp_labels)\n",
    "    print(f\"Shapiro-Wilk normality test (p-value): {shapiro_p:.2e}\")\n",
    "    print(f\"Distribution is {'normal' if shapiro_p > 0.05 else 'not normal'} (Œ±=0.05)\")\n",
    "    \n",
    "    # Skewness and kurtosis\n",
    "    skewness = stats.skew(sbp_labels)\n",
    "    kurt = stats.kurtosis(sbp_labels)\n",
    "    print(f\"Skewness: {skewness:.3f} ({'right-skewed' if skewness > 0 else 'left-skewed' if skewness < 0 else 'symmetric'})\")\n",
    "    print(f\"Kurtosis: {kurt:.3f} ({'leptokurtic' if kurt > 0 else 'platykurtic' if kurt < 0 else 'mesokurtic'})\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded.\")\n",
    "    print(\"This section will create blood pressure distribution visualizations when data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. DEMOGRAPHICS ANALYSIS & CORRELATIONS\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None and demographics_sbp is not None:\n",
    "    \n",
    "    # Extract demographics\n",
    "    age, gender, height, weight = demographics_sbp.T\n",
    "    height_m = height / 100\n",
    "    bmi = weight / (height_m ** 2)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig.suptitle('Demographics Analysis & Blood Pressure Correlations', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Age distribution\n",
    "    axes[0, 0].hist(age, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(np.mean(age), color='red', linestyle='--', label=f'Mean: {np.mean(age):.1f}')\n",
    "    axes[0, 0].set_xlabel('Age (years)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Age Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Age vs SBP correlation\n",
    "    axes[0, 1].scatter(age, sbp_labels, alpha=0.6, s=20)\n",
    "    # Add trend line\n",
    "    z = np.polyfit(age, sbp_labels, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[0, 1].plot(age, p(age), \"r--\", alpha=0.8)\n",
    "    corr_age_sbp = np.corrcoef(age, sbp_labels)[0, 1]\n",
    "    axes[0, 1].set_xlabel('Age (years)')\n",
    "    axes[0, 1].set_ylabel('SBP (mmHg)')\n",
    "    axes[0, 1].set_title(f'Age vs SBP (r = {corr_age_sbp:.3f})')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # BMI distribution\n",
    "    axes[1, 0].hist(bmi, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 0].axvline(np.mean(bmi), color='red', linestyle='--', label=f'Mean: {np.mean(bmi):.1f}')\n",
    "    # Add BMI category lines\n",
    "    axes[1, 0].axvline(18.5, color='orange', linestyle=':', label='Underweight')\n",
    "    axes[1, 0].axvline(25, color='orange', linestyle=':', label='Normal')\n",
    "    axes[1, 0].axvline(30, color='red', linestyle=':', label='Overweight')\n",
    "    axes[1, 0].set_xlabel('BMI (kg/m¬≤)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('BMI Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # BMI vs SBP correlation\n",
    "    axes[1, 1].scatter(bmi, sbp_labels, alpha=0.6, s=20, color='green')\n",
    "    z = np.polyfit(bmi, sbp_labels, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1, 1].plot(bmi, p(bmi), \"r--\", alpha=0.8)\n",
    "    corr_bmi_sbp = np.corrcoef(bmi, sbp_labels)[0, 1]\n",
    "    axes[1, 1].set_xlabel('BMI (kg/m¬≤)')\n",
    "    axes[1, 1].set_ylabel('SBP (mmHg)')\n",
    "    axes[1, 1].set_title(f'BMI vs SBP (r = {corr_bmi_sbp:.3f})')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gender comparison (Box plots)\n",
    "    male_sbp = sbp_labels[gender == 1]\n",
    "    female_sbp = sbp_labels[gender == 0]\n",
    "    \n",
    "    bp_data = [male_sbp, female_sbp]\n",
    "    bp = axes[2, 0].boxplot(bp_data, labels=['Male', 'Female'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['boxes'][1].set_facecolor('pink')\n",
    "    axes[2, 0].set_ylabel('SBP (mmHg)')\n",
    "    axes[2, 0].set_title('SBP by Gender')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistical test\n",
    "    from scipy.stats import ttest_ind\n",
    "    t_stat, t_p = ttest_ind(male_sbp, female_sbp)\n",
    "    axes[2, 0].text(0.5, 0.95, f't-test p-value: {t_p:.3f}', \n",
    "                    transform=axes[2, 0].transAxes, ha='center',\n",
    "                    bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "    \n",
    "    # Correlation matrix\n",
    "    demo_df = pd.DataFrame({\n",
    "        'Age': age,\n",
    "        'Height': height,\n",
    "        'Weight': weight,\n",
    "        'BMI': bmi,\n",
    "        'SBP': sbp_labels\n",
    "    })\n",
    "    \n",
    "    corr_matrix = demo_df.corr()\n",
    "    im = axes[2, 1].imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "    axes[2, 1].set_xticks(range(len(corr_matrix.columns)))\n",
    "    axes[2, 1].set_yticks(range(len(corr_matrix.columns)))\n",
    "    axes[2, 1].set_xticklabels(corr_matrix.columns, rotation=45)\n",
    "    axes[2, 1].set_yticklabels(corr_matrix.columns)\n",
    "    axes[2, 1].set_title('Correlation Matrix')\n",
    "    \n",
    "    # Add correlation values to the plot\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(len(corr_matrix.columns)):\n",
    "            axes[2, 1].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}', \n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=axes[2, 1], shrink=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä DEMOGRAPHICS & BP CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Age vs SBP correlation: {corr_age_sbp:.3f}\")\n",
    "    print(f\"BMI vs SBP correlation: {corr_bmi_sbp:.3f}\")\n",
    "    print(f\"Height vs SBP correlation: {np.corrcoef(height, sbp_labels)[0, 1]:.3f}\")\n",
    "    print(f\"Weight vs SBP correlation: {np.corrcoef(weight, sbp_labels)[0, 1]:.3f}\")\n",
    "    \n",
    "    print(f\"\\nGender comparison:\")\n",
    "    print(f\"Male SBP: {np.mean(male_sbp):.1f} ¬± {np.std(male_sbp):.1f} mmHg (n={len(male_sbp)})\")\n",
    "    print(f\"Female SBP: {np.mean(female_sbp):.1f} ¬± {np.std(female_sbp):.1f} mmHg (n={len(female_sbp)})\")\n",
    "    print(f\"Gender difference t-test p-value: {t_p:.3f}\")\n",
    "    \n",
    "    # BMI categories\n",
    "    underweight = np.sum(bmi < 18.5)\n",
    "    normal = np.sum((bmi >= 18.5) & (bmi < 25))\n",
    "    overweight = np.sum((bmi >= 25) & (bmi < 30))\n",
    "    obese = np.sum(bmi >= 30)\n",
    "    \n",
    "    print(f\"\\nBMI Categories:\")\n",
    "    print(f\"  Underweight (<18.5): {underweight} ({100*underweight/len(bmi):.1f}%)\")\n",
    "    print(f\"  Normal (18.5-24.9): {normal} ({100*normal/len(bmi):.1f}%)\")\n",
    "    print(f\"  Overweight (25-29.9): {overweight} ({100*overweight/len(bmi):.1f}%)\")\n",
    "    print(f\"  Obese (‚â•30): {obese} ({100*obese/len(bmi):.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded.\")\n",
    "    print(\"This section will show demographics analysis and correlations when data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. SIGNAL ANALYSIS & VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None:\n",
    "    from scipy.fft import fft, fftfreq\n",
    "    \n",
    "    # Select random samples for visualization\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    sample_indices = np.random.choice(len(signals_sbp), size=min(10, len(signals_sbp)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig.suptitle('Physiological Signal Analysis (PPG/ECG)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Signal examples (time domain)\n",
    "    time_axis = np.arange(signals_sbp.shape[1]) / 125  # Assuming 125 Hz sampling rate\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices[:5]):\n",
    "        color = plt.cm.tab10(i)\n",
    "        axes[0, 0].plot(time_axis, signals_sbp[idx], alpha=0.7, \n",
    "                       label=f'Sample {idx} (SBP: {sbp_labels[idx]:.0f})', color=color)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('Signal Amplitude')\n",
    "    axes[0, 0].set_title('Sample PPG/ECG Signals')\n",
    "    axes[0, 0].legend(loc='upper right', fontsize='small')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Signal statistics distribution\n",
    "    signal_means = np.mean(signals_sbp, axis=1)\n",
    "    signal_stds = np.std(signals_sbp, axis=1)\n",
    "    \n",
    "    axes[0, 1].hist(signal_means, bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Signal Mean')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Distribution of Signal Means')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Signal variance analysis\n",
    "    axes[1, 0].hist(signal_stds, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Signal Standard Deviation')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Distribution of Signal Variability')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Signal amplitude range\n",
    "    signal_ranges = np.max(signals_sbp, axis=1) - np.min(signals_sbp, axis=1)\n",
    "    axes[1, 1].hist(signal_ranges, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Signal Range (Max - Min)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Distribution of Signal Amplitude Ranges')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Frequency domain analysis (FFT of sample signals)\n",
    "    sample_signal = signals_sbp[sample_indices[0]]\n",
    "    fft_vals = np.abs(fft(sample_signal))\n",
    "    freqs = fftfreq(len(sample_signal), 1/125)  # 125 Hz sampling rate\n",
    "    \n",
    "    # Plot only positive frequencies up to Nyquist\n",
    "    pos_mask = freqs >= 0\n",
    "    axes[2, 0].plot(freqs[pos_mask][:len(freqs)//2], \n",
    "                   fft_vals[pos_mask][:len(freqs)//2])\n",
    "    axes[2, 0].set_xlabel('Frequency (Hz)')\n",
    "    axes[2, 0].set_ylabel('FFT Magnitude')\n",
    "    axes[2, 0].set_title('Frequency Domain Analysis (Sample Signal)')\n",
    "    axes[2, 0].set_xlim(0, 20)  # Focus on physiologically relevant frequencies\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Signal quality metrics\n",
    "    # Calculate SNR-like metric (signal power vs noise estimation)\n",
    "    signal_power = np.var(signals_sbp, axis=1)\n",
    "    noise_est = np.std(np.diff(signals_sbp, axis=1), axis=1)  # High-frequency noise estimate\n",
    "    snr_estimate = signal_power / (noise_est**2 + 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    axes[2, 1].hist(np.log10(snr_estimate), bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[2, 1].set_xlabel('Signal Quality (log10 SNR estimate)')\n",
    "    axes[2, 1].set_ylabel('Frequency')\n",
    "    axes[2, 1].set_title('Signal Quality Distribution')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Signal statistics summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä SIGNAL CHARACTERISTICS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Signal length: {signals_sbp.shape[1]} samples\")\n",
    "    print(f\"Estimated duration: {signals_sbp.shape[1]/125:.1f} seconds (@ 125 Hz)\")\n",
    "    print(f\"Signal amplitude range: {np.min(signals_sbp):.3f} to {np.max(signals_sbp):.3f}\")\n",
    "    print(f\"Mean signal amplitude: {np.mean(signal_means):.3f} ¬± {np.std(signal_means):.3f}\")\n",
    "    print(f\"Mean signal variability: {np.mean(signal_stds):.3f} ¬± {np.std(signal_stds):.3f}\")\n",
    "    print(f\"Average amplitude range: {np.mean(signal_ranges):.3f} ¬± {np.std(signal_ranges):.3f}\")\n",
    "    \n",
    "    # Correlation between signal characteristics and BP\n",
    "    corr_mean_bp = np.corrcoef(signal_means, sbp_labels)[0, 1]\n",
    "    corr_std_bp = np.corrcoef(signal_stds, sbp_labels)[0, 1]\n",
    "    corr_range_bp = np.corrcoef(signal_ranges, sbp_labels)[0, 1]\n",
    "    \n",
    "    print(f\"\\nSignal-BP Correlations:\")\n",
    "    print(f\"  Signal mean vs SBP: r = {corr_mean_bp:.3f}\")\n",
    "    print(f\"  Signal std vs SBP: r = {corr_std_bp:.3f}\")\n",
    "    print(f\"  Signal range vs SBP: r = {corr_range_bp:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No signal data loaded.\")\n",
    "    print(\"This section will analyze PPG/ECG signal characteristics when data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c14c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. ADVANCED SIGNAL FEATURES & PULSE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None:\n",
    "    from scipy.signal import find_peaks, welch\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Advanced Signal Feature Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract features from a sample signal\n",
    "    sample_signal = signals_sbp[sample_indices[0]]\n",
    "    \n",
    "    # 1. Peak detection analysis\n",
    "    peaks, _ = find_peaks(sample_signal, height=np.percentile(sample_signal, 70))\n",
    "    axes[0, 0].plot(time_axis, sample_signal, alpha=0.8, label='Signal')\n",
    "    axes[0, 0].plot(time_axis[peaks], sample_signal[peaks], \"rx\", label=f'Peaks ({len(peaks)})')\n",
    "    axes[0, 0].set_xlabel('Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    axes[0, 0].set_title('Peak Detection (Heart Rate Estimation)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate heart rate\n",
    "    if len(peaks) > 1:\n",
    "        peak_intervals = np.diff(peaks) / 125  # Convert to seconds\n",
    "        heart_rate = 60 / np.mean(peak_intervals)  # BPM\n",
    "        axes[0, 0].text(0.02, 0.98, f'Est. HR: {heart_rate:.1f} BPM', \n",
    "                       transform=axes[0, 0].transAxes, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
    "    \n",
    "    # 2. Power Spectral Density\n",
    "    freqs, psd = welch(sample_signal, fs=125, nperseg=min(256, len(sample_signal)//4))\n",
    "    axes[0, 1].semilogy(freqs, psd)\n",
    "    axes[0, 1].set_xlabel('Frequency (Hz)')\n",
    "    axes[0, 1].set_ylabel('Power Spectral Density')\n",
    "    axes[0, 1].set_title('Power Spectral Density')\n",
    "    axes[0, 1].set_xlim(0, 10)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Signal derivatives (for morphological analysis)\n",
    "    signal_derivative = np.gradient(sample_signal)\n",
    "    axes[0, 2].plot(time_axis, signal_derivative, color='orange')\n",
    "    axes[0, 2].set_xlabel('Time (seconds)')\n",
    "    axes[0, 2].set_ylabel('Signal Derivative')\n",
    "    axes[0, 2].set_title('Signal First Derivative')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Heart Rate Variability Analysis (for all signals)\n",
    "    heart_rates = []\n",
    "    hrv_metrics = []\n",
    "    \n",
    "    for i, signal in enumerate(signals_sbp[:min(100, len(signals_sbp))]):  # Analyze first 100 signals\n",
    "        peaks, _ = find_peaks(signal, height=np.percentile(signal, 70))\n",
    "        if len(peaks) > 1:\n",
    "            peak_intervals = np.diff(peaks) / 125\n",
    "            hr = 60 / np.mean(peak_intervals)\n",
    "            heart_rates.append(hr)\n",
    "            \n",
    "            # HRV metric: RMSSD (root mean square of successive differences)\n",
    "            if len(peak_intervals) > 1:\n",
    "                rmssd = np.sqrt(np.mean(np.diff(peak_intervals)**2)) * 1000  # ms\n",
    "                hrv_metrics.append(rmssd)\n",
    "    \n",
    "    if heart_rates:\n",
    "        axes[1, 0].hist(heart_rates, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "        axes[1, 0].set_xlabel('Heart Rate (BPM)')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Heart Rate Distribution')\n",
    "        axes[1, 0].axvline(np.mean(heart_rates), color='red', linestyle='--', \n",
    "                          label=f'Mean: {np.mean(heart_rates):.1f} BPM')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Signal morphology features\n",
    "    # Calculate pulse width, amplitude ratios, etc.\n",
    "    pulse_widths = []\n",
    "    amplitude_ratios = []\n",
    "    \n",
    "    for i, signal in enumerate(signals_sbp[:min(100, len(signals_sbp))]):\n",
    "        peaks, peak_props = find_peaks(signal, height=np.percentile(signal, 70), width=1)\n",
    "        if len(peaks) > 0:\n",
    "            # Pulse width at half maximum\n",
    "            if 'widths' in peak_props:\n",
    "                pulse_widths.extend(peak_props['widths'] / 125)  # Convert to seconds\n",
    "            \n",
    "            # Amplitude ratio (systolic peak to baseline)\n",
    "            baseline = np.percentile(signal, 10)\n",
    "            peak_amplitude = signal[peaks] - baseline\n",
    "            amplitude_ratios.extend(peak_amplitude)\n",
    "    \n",
    "    if pulse_widths:\n",
    "        axes[1, 1].hist(pulse_widths, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[1, 1].set_xlabel('Pulse Width (seconds)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title('Pulse Width Distribution')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Signal quality vs BP correlation\n",
    "    if heart_rates and len(heart_rates) == len(sbp_labels[:len(heart_rates)]):\n",
    "        hr_bp_corr = np.corrcoef(heart_rates, sbp_labels[:len(heart_rates)])[0, 1]\n",
    "        axes[1, 2].scatter(heart_rates, sbp_labels[:len(heart_rates)], alpha=0.6)\n",
    "        axes[1, 2].set_xlabel('Heart Rate (BPM)')\n",
    "        axes[1, 2].set_ylabel('SBP (mmHg)')\n",
    "        axes[1, 2].set_title(f'Heart Rate vs SBP (r = {hr_bp_corr:.3f})')\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(heart_rates, sbp_labels[:len(heart_rates)], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[1, 2].plot(heart_rates, p(heart_rates), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Advanced metrics summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç ADVANCED SIGNAL METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if heart_rates:\n",
    "        print(f\"Heart Rate Analysis (n={len(heart_rates)} signals):\")\n",
    "        print(f\"  Mean HR: {np.mean(heart_rates):.1f} ¬± {np.std(heart_rates):.1f} BPM\")\n",
    "        print(f\"  HR Range: {np.min(heart_rates):.1f} - {np.max(heart_rates):.1f} BPM\")\n",
    "        print(f\"  HR vs SBP correlation: r = {hr_bp_corr:.3f}\")\n",
    "        \n",
    "    if hrv_metrics:\n",
    "        print(f\"\\nHeart Rate Variability (RMSSD):\")\n",
    "        print(f\"  Mean: {np.mean(hrv_metrics):.1f} ¬± {np.std(hrv_metrics):.1f} ms\")\n",
    "        \n",
    "    if pulse_widths:\n",
    "        print(f\"\\nPulse Morphology:\")\n",
    "        print(f\"  Mean pulse width: {np.mean(pulse_widths):.3f} ¬± {np.std(pulse_widths):.3f} seconds\")\n",
    "        \n",
    "    if amplitude_ratios:\n",
    "        print(f\"  Mean pulse amplitude: {np.mean(amplitude_ratios):.3f} ¬± {np.std(amplitude_ratios):.3f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No signal data loaded.\")\n",
    "    print(\"This section will perform advanced signal feature analysis when data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. DATA QUALITY ASSESSMENT & OUTLIER DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None and sbp_labels is not None:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Data Quality Assessment & Outlier Detection', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Missing values check (should be none for loaded data, but good practice)\n",
    "    missing_signals = np.sum(np.isnan(signals_sbp), axis=1)\n",
    "    missing_demographics = np.sum(np.isnan(demographics_sbp), axis=1)\n",
    "    \n",
    "    axes[0, 0].hist(missing_signals, bins=20, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Missing Values per Signal')\\n    axes[0, 0].set_ylabel('Frequency')\\n    axes[0, 0].set_title('Missing Values in Signals')\\n    axes[0, 0].grid(True, alpha=0.3)\\n    \\n    # Add text annotation\\n    total_missing = np.sum(missing_signals > 0)\\n    axes[0, 0].text(0.7, 0.8, f'Signals with missing values: {total_missing}', \\n                    transform=axes[0, 0].transAxes, \\n                    bbox=dict(boxstyle=\\\"round\\\", facecolor='wheat'))\\n    \\n    # 2. Signal amplitude outliers\\n    signal_max = np.max(signals_sbp, axis=1)\\n    signal_min = np.min(signals_sbp, axis=1)\\n    \\n    axes[0, 1].scatter(signal_min, signal_max, alpha=0.6, s=20)\\n    axes[0, 1].set_xlabel('Signal Minimum')\\n    axes[0, 1].set_ylabel('Signal Maximum')\\n    axes[0, 1].set_title('Signal Amplitude Range')\\n    axes[0, 1].grid(True, alpha=0.3)\\n    \\n    # Identify extreme outliers (beyond 3 standard deviations)\\n    z_max = np.abs((signal_max - np.mean(signal_max)) / np.std(signal_max))\\n    z_min = np.abs((signal_min - np.mean(signal_min)) / np.std(signal_min))\\n    amplitude_outliers = (z_max > 3) | (z_min > 3)\\n    \\n    axes[0, 1].scatter(signal_min[amplitude_outliers], signal_max[amplitude_outliers], \\n                      color='red', s=30, label=f'Outliers ({np.sum(amplitude_outliers)})')\\n    axes[0, 1].legend()\\n    \\n    # 3. Blood pressure outliers\\n    # Using IQR method for BP outliers\\n    Q1_bp = np.percentile(sbp_labels, 25)\\n    Q3_bp = np.percentile(sbp_labels, 75)\\n    IQR_bp = Q3_bp - Q1_bp\\n    bp_outliers = (sbp_labels < (Q1_bp - 1.5 * IQR_bp)) | (sbp_labels > (Q3_bp + 1.5 * IQR_bp))\\n    \\n    axes[0, 2].hist(sbp_labels, bins=50, alpha=0.7, color='lightgreen', edgecolor='black', label='Normal')\\n    axes[0, 2].hist(sbp_labels[bp_outliers], bins=20, alpha=0.8, color='red', edgecolor='black', label='Outliers')\\n    axes[0, 2].axvline(Q1_bp - 1.5 * IQR_bp, color='red', linestyle='--', alpha=0.7, label='Lower bound')\\n    axes[0, 2].axvline(Q3_bp + 1.5 * IQR_bp, color='red', linestyle='--', alpha=0.7, label='Upper bound')\\n    axes[0, 2].set_xlabel('Systolic Blood Pressure (mmHg)')\\n    axes[0, 2].set_ylabel('Frequency')\\n    axes[0, 2].set_title(f'BP Outliers ({np.sum(bp_outliers)} samples)')\\n    axes[0, 2].legend()\\n    axes[0, 2].grid(True, alpha=0.3)\\n    \\n    # 4. Multivariate outlier detection using Isolation Forest\\n    # Combine signal features and demographics\\n    features = np.column_stack([\\n        np.mean(signals_sbp, axis=1),\\n        np.std(signals_sbp, axis=1),\\n        np.max(signals_sbp, axis=1) - np.min(signals_sbp, axis=1),\\n        demographics_sbp[:, 0],  # Age\\n        demographics_sbp[:, 2],  # Height\\n        demographics_sbp[:, 3],  # Weight\\n        sbp_labels\\n    ])\\n    \\n    # Standardize features\\n    scaler = StandardScaler()\\n    features_scaled = scaler.fit_transform(features)\\n    \\n    # Apply Isolation Forest\\n    iso_forest = IsolationForest(contamination=0.1, random_state=42)  # Expect 10% outliers\\n    outlier_labels = iso_forest.fit_predict(features_scaled)\\n    outliers = outlier_labels == -1\\n    \\n    # Plot age vs SBP with outliers highlighted\\n    age = demographics_sbp[:, 0]\\n    axes[1, 0].scatter(age[~outliers], sbp_labels[~outliers], alpha=0.6, s=20, \\n                      color='blue', label=f'Normal ({np.sum(~outliers)})')\\n    axes[1, 0].scatter(age[outliers], sbp_labels[outliers], alpha=0.8, s=30, \\n                      color='red', label=f'Outliers ({np.sum(outliers)})')\\n    axes[1, 0].set_xlabel('Age (years)')\\n    axes[1, 0].set_ylabel('SBP (mmHg)')\\n    axes[1, 0].set_title('Multivariate Outliers (Age vs SBP)')\\n    axes[1, 0].legend()\\n    axes[1, 0].grid(True, alpha=0.3)\\n    \\n    # 5. Signal noise analysis\\n    # High-frequency noise estimation using signal derivatives\\n    signal_noise = np.std(np.diff(signals_sbp, axis=1), axis=1)\\n    \\n    axes[1, 1].hist(signal_noise, bins=50, alpha=0.7, color='orange', edgecolor='black')\\n    axes[1, 1].set_xlabel('Estimated Noise Level (Std of derivatives)')\\n    axes[1, 1].set_ylabel('Frequency')\\n    axes[1, 1].set_title('Signal Noise Distribution')\\n    axes[1, 1].grid(True, alpha=0.3)\\n    \\n    # Identify noisy signals\\n    noise_threshold = np.percentile(signal_noise, 95)  # Top 5% as noisy\\n    noisy_signals = signal_noise > noise_threshold\\n    axes[1, 1].axvline(noise_threshold, color='red', linestyle='--', \\n                      label=f'95th percentile ({np.sum(noisy_signals)} signals)')\\n    axes[1, 1].legend()\\n    \\n    # 6. Data completeness heatmap\\n    # Create a sample of data completeness (for visualization)\\n    sample_size = min(100, len(signals_sbp))\\n    sample_indices = np.random.choice(len(signals_sbp), sample_size, replace=False)\\n    \\n    # Check for zeros or constant segments in signals (potential data quality issues)\\n    completeness_matrix = np.zeros((sample_size, 4))  # 4 quality metrics\\n    \\n    for i, idx in enumerate(sample_indices):\\n        signal = signals_sbp[idx]\\n        # Metric 1: Non-zero ratio\\n        completeness_matrix[i, 0] = np.mean(signal != 0)\\n        # Metric 2: Non-constant ratio (signal variability)\\n        completeness_matrix[i, 1] = 1 - (np.sum(signal == signal[0]) / len(signal))\\n        # Metric 3: Reasonable amplitude (not clipped)\\n        completeness_matrix[i, 2] = 1 - ((np.sum(signal == np.max(signal)) + \\n                                         np.sum(signal == np.min(signal))) / len(signal))\\n        # Metric 4: Demographics completeness\\n        completeness_matrix[i, 3] = 1 - (np.sum(np.isnan(demographics_sbp[idx])) / len(demographics_sbp[idx]))\\n    \\n    im = axes[1, 2].imshow(completeness_matrix.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\\n    axes[1, 2].set_xlabel('Sample Index')\\n    axes[1, 2].set_ylabel('Quality Metrics')\\n    axes[1, 2].set_title('Data Quality Heatmap (Sample)')\\n    axes[1, 2].set_yticks([0, 1, 2, 3])\\n    axes[1, 2].set_yticklabels(['Non-zero', 'Variable', 'Not-clipped', 'Demo-complete'])\\n    \\n    # Add colorbar\\n    plt.colorbar(im, ax=axes[1, 2], shrink=0.8)\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    # Data quality summary\\n    print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n    print(\\\"üîç DATA QUALITY ASSESSMENT SUMMARY\\\")\\n    print(\\\"=\\\"*60)\\n    \\n    print(f\\\"Total samples: {len(signals_sbp):,}\\\")\\n    print(f\\\"Signals with missing values: {np.sum(missing_signals > 0)}\\\")\\n    print(f\\\"Demographics with missing values: {np.sum(missing_demographics > 0)}\\\")\\n    \\n    print(f\\\"\\\\nOutlier Detection:\\\")\\n    print(f\\\"  Signal amplitude outliers: {np.sum(amplitude_outliers)} ({100*np.sum(amplitude_outliers)/len(signals_sbp):.1f}%)\\\")\\n    print(f\\\"  Blood pressure outliers (IQR): {np.sum(bp_outliers)} ({100*np.sum(bp_outliers)/len(sbp_labels):.1f}%)\\\")\\n    print(f\\\"  Multivariate outliers (Isolation Forest): {np.sum(outliers)} ({100*np.sum(outliers)/len(signals_sbp):.1f}%)\\\")\\n    print(f\\\"  Noisy signals (top 5%): {np.sum(noisy_signals)} ({100*np.sum(noisy_signals)/len(signals_sbp):.1f}%)\\\")\\n    \\n    print(f\\\"\\\\nSignal Quality Metrics:\\\")\\n    print(f\\\"  Mean noise level: {np.mean(signal_noise):.4f} ¬± {np.std(signal_noise):.4f}\\\")\\n    print(f\\\"  Average signal completeness: {np.mean(completeness_matrix):.3f}\\\")\\n    \\n    # Recommendations\\n    print(f\\\"\\\\nüí° Data Quality Recommendations:\\\")\\n    if np.sum(amplitude_outliers) > len(signals_sbp) * 0.05:\\n        print(f\\\"  ‚ö†Ô∏è  High number of amplitude outliers - consider signal normalization\\\")\\n    if np.sum(noisy_signals) > len(signals_sbp) * 0.1:\\n        print(f\\\"  ‚ö†Ô∏è  Many noisy signals detected - apply filtering in preprocessing\\\")\\n    if np.sum(bp_outliers) > len(sbp_labels) * 0.05:\\n        print(f\\\"  ‚ö†Ô∏è  Significant BP outliers - verify measurement accuracy\\\")\\n    if np.mean(completeness_matrix) < 0.95:\\n        print(f\\\"  ‚ö†Ô∏è  Data completeness issues detected - investigate data collection\\\")\\n        \\nelse:\\n    print(\\\"‚ö†Ô∏è  No data loaded.\\\")\\n    print(\\\"This section will assess data quality and detect outliers when data is available.\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b48ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. FEATURE ENGINEERING & PREPROCESSING INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None:\n",
    "    from scipy.signal import butter, filtfilt, resample\n",
    "    from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "    from scipy.fft import fft, fftfreq\n",
    "    from scipy import stats\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig.suptitle('Feature Engineering & Preprocessing Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Select a representative signal for preprocessing demonstration\n",
    "    demo_signal = signals_sbp[sample_indices[0]].copy()\n",
    "    original_signal = demo_signal.copy()\n",
    "    \n",
    "    # 1. Signal filtering effects\n",
    "    # Design a bandpass filter (typical for PPG: 0.5-8 Hz)\n",
    "    nyquist = 125 / 2  # Half of sampling frequency\n",
    "    low_cut = 0.5 / nyquist\n",
    "    high_cut = 8.0 / nyquist\n",
    "    b, a = butter(4, [low_cut, high_cut], btype='band')\n",
    "    filtered_signal = filtfilt(b, a, demo_signal)\n",
    "    \n",
    "    axes[0, 0].plot(time_axis, original_signal, alpha=0.7, label='Original', linewidth=2)\n",
    "    axes[0, 0].plot(time_axis, filtered_signal, alpha=0.8, label='Filtered (0.5-8 Hz)', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('Signal Amplitude')\n",
    "    axes[0, 0].set_title('Effect of Bandpass Filtering')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Normalization comparison\n",
    "    # Min-Max normalization\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    signal_minmax = minmax_scaler.fit_transform(demo_signal.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Z-score normalization\n",
    "    signal_zscore = (demo_signal - np.mean(demo_signal)) / np.std(demo_signal)\n",
    "    \n",
    "    # Robust normalization (median-based)\n",
    "    robust_scaler = RobustScaler()\n",
    "    signal_robust = robust_scaler.fit_transform(demo_signal.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    axes[0, 1].plot(time_axis, signal_minmax, alpha=0.8, label='Min-Max [0,1]', linewidth=2)\n",
    "    axes[0, 1].plot(time_axis, signal_zscore, alpha=0.8, label='Z-score', linewidth=2)\n",
    "    axes[0, 1].plot(time_axis, signal_robust, alpha=0.8, label='Robust (IQR)', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Time (seconds)')\n",
    "    axes[0, 1].set_ylabel('Normalized Amplitude')\n",
    "    axes[0, 1].set_title('Normalization Methods Comparison')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Signal length distribution and resampling analysis\n",
    "    signal_lengths = [len(sig) for sig in signals_sbp]\n",
    "    axes[1, 0].hist(signal_lengths, bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Signal Length (samples)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Signal Length Distribution')\n",
    "    axes[1, 0].axvline(np.mean(signal_lengths), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(signal_lengths):.0f}')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Resampling demonstration (if signals have different lengths)\n",
    "    target_length = int(np.median(signal_lengths))\n",
    "    if len(demo_signal) != target_length:\n",
    "        resampled_signal = resample(demo_signal, target_length)\n",
    "        time_resampled = np.arange(target_length) / 125\n",
    "        \n",
    "        axes[1, 1].plot(time_axis, demo_signal, alpha=0.7, label=f'Original ({len(demo_signal)} samples)', linewidth=2)\n",
    "        axes[1, 1].plot(time_resampled, resampled_signal, alpha=0.8, \n",
    "                       label=f'Resampled ({target_length} samples)', linewidth=2)\n",
    "    else:\n",
    "        axes[1, 1].plot(time_axis, demo_signal, alpha=0.7, label='Signal (already target length)', linewidth=2)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Signal Amplitude')\n",
    "    axes[1, 1].set_title('Signal Resampling')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Feature extraction demonstration\n",
    "    # Extract various time-domain and frequency-domain features\n",
    "    n_samples = min(100, len(signals_sbp))\n",
    "    feature_matrix = np.zeros((n_samples, 10))  # 10 features\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        signal = signals_sbp[i]\n",
    "        \n",
    "        # Time domain features\n",
    "        feature_matrix[i, 0] = np.mean(signal)  # Mean\n",
    "        feature_matrix[i, 1] = np.std(signal)   # Standard deviation\n",
    "        feature_matrix[i, 2] = np.max(signal) - np.min(signal)  # Range\n",
    "        feature_matrix[i, 3] = np.sqrt(np.mean(signal**2))  # RMS\n",
    "        feature_matrix[i, 4] = stats.skew(signal)  # Skewness\n",
    "        \n",
    "        # Frequency domain features (simple spectral analysis)\n",
    "        fft_vals = np.abs(fft(signal))\n",
    "        freqs = fftfreq(len(signal), 1/125)\n",
    "        pos_freqs = freqs[:len(freqs)//2]\n",
    "        pos_fft = fft_vals[:len(fft_vals)//2]\n",
    "        \n",
    "        # Spectral centroid (weighted mean frequency)\n",
    "        feature_matrix[i, 5] = np.sum(pos_freqs * pos_fft) / np.sum(pos_fft) if np.sum(pos_fft) > 0 else 0\n",
    "        \n",
    "        # Spectral energy in different bands\n",
    "        low_band = (pos_freqs >= 0.5) & (pos_freqs < 2.0)\n",
    "        mid_band = (pos_freqs >= 2.0) & (pos_freqs < 5.0)\n",
    "        high_band = (pos_freqs >= 5.0) & (pos_freqs < 8.0)\n",
    "        \n",
    "        feature_matrix[i, 6] = np.sum(pos_fft[low_band])   # Low frequency energy\n",
    "        feature_matrix[i, 7] = np.sum(pos_fft[mid_band])   # Mid frequency energy\n",
    "        feature_matrix[i, 8] = np.sum(pos_fft[high_band])  # High frequency energy\n",
    "        \n",
    "        # Signal complexity (approximate entropy)\n",
    "        feature_matrix[i, 9] = np.std(np.diff(signal))  # First difference std as complexity measure\n",
    "    \n",
    "    # Plot feature correlation with SBP\n",
    "    feature_names = ['Mean', 'Std', 'Range', 'RMS', 'Skewness', \n",
    "                    'Spec.Centroid', 'Low Freq', 'Mid Freq', 'High Freq', 'Complexity']\n",
    "    \n",
    "    correlations = []\n",
    "    for j in range(feature_matrix.shape[1]):\n",
    "        corr = np.corrcoef(feature_matrix[:, j], sbp_labels[:n_samples])[0, 1]\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    bars = axes[2, 0].bar(range(len(correlations)), correlations, \n",
    "                          color=['red' if abs(c) > 0.1 else 'lightblue' for c in correlations])\n",
    "    axes[2, 0].set_xlabel('Features')\n",
    "    axes[2, 0].set_ylabel('Correlation with SBP')\n",
    "    axes[2, 0].set_title('Feature-SBP Correlations')\n",
    "    axes[2, 0].set_xticks(range(len(feature_names)))\n",
    "    axes[2, 0].set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    axes[2, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add correlation values on bars\n",
    "    for bar, corr in zip(bars, correlations):\n",
    "        height = bar.get_height()\n",
    "        axes[2, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{corr:.3f}', ha='center', va='bottom' if corr > 0 else 'top')\n",
    "    \n",
    "    # 6. Feature distribution comparison\n",
    "    # Show distribution of most correlated feature\n",
    "    best_feature_idx = np.argmax(np.abs(correlations))\n",
    "    best_feature = feature_matrix[:, best_feature_idx]\n",
    "    \n",
    "    axes[2, 1].scatter(best_feature, sbp_labels[:n_samples], alpha=0.6)\n",
    "    axes[2, 1].set_xlabel(f'{feature_names[best_feature_idx]}')\n",
    "    axes[2, 1].set_ylabel('SBP (mmHg)')\n",
    "    axes[2, 1].set_title(f'Best Feature vs SBP (r = {correlations[best_feature_idx]:.3f})')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(best_feature, sbp_labels[:n_samples], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[2, 1].plot(best_feature, p(best_feature), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature engineering summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîß FEATURE ENGINEERING & PREPROCESSING INSIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"Signal characteristics:\")\n",
    "    print(f\"  Original signal range: [{np.min(original_signal):.3f}, {np.max(original_signal):.3f}]\")\n",
    "    print(f\"  After filtering: [{np.min(filtered_signal):.3f}, {np.max(filtered_signal):.3f}]\")\n",
    "    print(f\"  Signal length statistics: {np.mean(signal_lengths):.0f} ¬± {np.std(signal_lengths):.0f} samples\")\n",
    "    \n",
    "    print(f\"\\nFeature correlations with SBP:\")\n",
    "    for i, (name, corr) in enumerate(zip(feature_names, correlations)):\n",
    "        significance = \"***\" if abs(corr) > 0.3 else \"**\" if abs(corr) > 0.2 else \"*\" if abs(corr) > 0.1 else \"\"\n",
    "        print(f\"  {name:15s}: {corr:6.3f} {significance}\")\n",
    "    \n",
    "    print(f\"\\nPreprocessing recommendations:\")\n",
    "    print(f\"  üîÑ Apply bandpass filtering (0.5-8 Hz) to remove noise and artifacts\")\n",
    "    if np.std(signal_lengths) > np.mean(signal_lengths) * 0.1:\n",
    "        print(f\"  üìè Standardize signal length to {int(np.median(signal_lengths))} samples\")\n",
    "    print(f\"  üìä Use robust normalization to handle outliers\")\n",
    "    print(f\"  üéØ Focus on features with |correlation| > 0.1 for model training\")\n",
    "    \n",
    "    # Identify best features\n",
    "    good_features = [feature_names[i] for i, c in enumerate(correlations) if abs(c) > 0.1]\n",
    "    if good_features:\n",
    "        print(f\"  ‚≠ê Promising features: {', '.join(good_features)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded.\")\n",
    "    print(\"This section will analyze feature engineering and preprocessing when data is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed7956",
   "metadata": {},
   "source": [
    "## üìã Summary & Conclusions\n",
    "\n",
    "This comprehensive EDA notebook provides insights into the cuffless blood pressure dataset for model development:\n",
    "\n",
    "### Key Findings:\n",
    "- **Dataset Overview**: Complete statistics on signal characteristics, BP distributions, and demographic patterns\n",
    "- **Signal Quality**: Assessment of noise levels, outliers, and data completeness\n",
    "- **Feature Correlations**: Identification of features most correlated with blood pressure\n",
    "- **Preprocessing Insights**: Optimal filtering, normalization, and feature engineering strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
