{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f5ec82",
   "metadata": {},
   "source": [
    "# Cuffless Blood Pressure - Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis for the cuffless blood pressure estimation project. The analysis covers:\n",
    "\n",
    "## üìä EDA Structure\n",
    "\n",
    "1. **Data Loading & Overview** - Dataset statistics and basic information\n",
    "2. **Signal Analysis** - PPG/ECG signal characteristics and quality assessment  \n",
    "3. **Blood Pressure Distribution** - Systolic and diastolic BP patterns\n",
    "4. **Demographics Analysis** - Age, gender, height, weight distributions and correlations\n",
    "5. **Signal Quality Assessment** - Noise detection, signal-to-noise ratio analysis\n",
    "6. **Feature Relationships** - Correlations between demographics and BP values\n",
    "7. **Signal Visualization** - Time-domain and frequency-domain analysis\n",
    "8. **Data Quality Checks** - Missing values, outliers, and anomaly detection\n",
    "9. **Statistical Analysis** - Hypothesis testing and significance analysis\n",
    "10. **Preprocessing Insights** - Signal characteristics for model preparation\n",
    "\n",
    "## üéØ Objective\n",
    "Understand the PulseDB dataset characteristics to inform preprocessing decisions and model architecture choices for accurate blood pressure estimation from physiological signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once only\n",
    "# %pip install --upgrade pip\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb73a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the processed data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for processed data\n",
    "processed_dir = Path('../data/processed')\n",
    "npy_files = list(processed_dir.glob('*.npy'))\n",
    "\n",
    "if npy_files:\n",
    "    print(f\"üìä Found {len(npy_files)} processed data files:\")\n",
    "    for file in npy_files:\n",
    "        print(f\"   - {file.name}\")\n",
    "    \n",
    "    # Load the data\n",
    "    try:\n",
    "        signals_sbp = np.load(processed_dir / 'signals_sbp.npy')\n",
    "        sbp_labels = np.load(processed_dir / 'sbp_labels.npy')\n",
    "        demographics_sbp = np.load(processed_dir / 'demographics_sbp.npy')\n",
    "        \n",
    "        print(f\"\\nüìà Dataset Statistics:\")\n",
    "        print(f\"Number of samples: {len(signals_sbp)}\")\n",
    "        print(f\"Signal length: {signals_sbp.shape[1]}\")\n",
    "        print(f\"Demographics features: {demographics_sbp.shape[1]} (Age, Gender, Height, Weight)\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Processed .npy files not found. Please run the preprocessing notebook first.\")\n",
    "        signals_sbp = sbp_labels = demographics_sbp = None\n",
    "else:\n",
    "    print(\"‚ùå No processed data found. Please run the preprocessing notebook first.\")\n",
    "    signals_sbp = sbp_labels = demographics_sbp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead41ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DATASET OVERVIEW & BASIC STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None and sbp_labels is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìà DATASET OVERVIEW\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Dataset size: {len(signals_sbp):,} samples\")\n",
    "    print(f\"Signal length: {signals_sbp.shape[1]:,} data points\")\n",
    "    print(f\"Signal duration: ~{signals_sbp.shape[1]/125:.1f} seconds (assuming 125 Hz)\")\n",
    "    print(f\"Demographics features: {demographics_sbp.shape[1]} (Age, Gender, Height, Weight)\")\n",
    "    \n",
    "    # Memory usage\n",
    "    signal_size_mb = signals_sbp.nbytes / (1024**2)\n",
    "    total_size_mb = (signals_sbp.nbytes + sbp_labels.nbytes + demographics_sbp.nbytes) / (1024**2)\n",
    "    print(f\"Memory usage: {total_size_mb:.1f} MB (signals: {signal_size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üî¢ BLOOD PRESSURE STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # SBP statistics\n",
    "    print(\"Systolic Blood Pressure (SBP):\")\n",
    "    print(f\"  Mean: {np.mean(sbp_labels):.1f} ¬± {np.std(sbp_labels):.1f} mmHg\")\n",
    "    print(f\"  Range: {np.min(sbp_labels):.1f} - {np.max(sbp_labels):.1f} mmHg\")\n",
    "    print(f\"  Median: {np.median(sbp_labels):.1f} mmHg\")\n",
    "    print(f\"  25th percentile: {np.percentile(sbp_labels, 25):.1f} mmHg\")\n",
    "    print(f\"  75th percentile: {np.percentile(sbp_labels, 75):.1f} mmHg\")\n",
    "    \n",
    "    # BP categories (American Heart Association guidelines)\n",
    "    normal = np.sum(sbp_labels < 120)\n",
    "    elevated = np.sum((sbp_labels >= 120) & (sbp_labels < 130))\n",
    "    stage1 = np.sum((sbp_labels >= 130) & (sbp_labels < 140))\n",
    "    stage2 = np.sum(sbp_labels >= 140)\n",
    "    \n",
    "    print(\"\\nüìä BP Categories (AHA Guidelines):\")\n",
    "    print(f\"  Normal (<120): {normal:,} ({100*normal/len(sbp_labels):.1f}%)\")\n",
    "    print(f\"  Elevated (120-129): {elevated:,} ({100*elevated/len(sbp_labels):.1f}%)\")\n",
    "    print(f\"  Stage 1 (130-139): {stage1:,} ({100*stage1/len(sbp_labels):.1f}%)\")\n",
    "    print(f\"  Stage 2 (‚â•140): {stage2:,} ({100*stage2/len(sbp_labels):.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üë• DEMOGRAPHICS OVERVIEW\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Demographics statistics\n",
    "    age, gender, height, weight = demographics_sbp.T\n",
    "    \n",
    "    print(\"Age:\")\n",
    "    print(f\"  Mean: {np.mean(age):.1f} ¬± {np.std(age):.1f} years\")\n",
    "    print(f\"  Range: {np.min(age):.0f} - {np.max(age):.0f} years\")\n",
    "    \n",
    "    print(\"\\nGender Distribution:\")\n",
    "    male_count = np.sum(gender == 1)\n",
    "    female_count = np.sum(gender == 0)\n",
    "    print(f\"  Male: {male_count:,} ({100*male_count/len(gender):.1f}%)\")\n",
    "    print(f\"  Female: {female_count:,} ({100*female_count/len(gender):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nHeight: {np.mean(height):.1f} ¬± {np.std(height):.1f} cm\")\n",
    "    print(f\"Weight: {np.mean(weight):.1f} ¬± {np.std(weight):.1f} kg\")\n",
    "    \n",
    "    # BMI calculation\n",
    "    height_m = height / 100  # convert cm to meters\n",
    "    bmi = weight / (height_m ** 2)\n",
    "    print(f\"BMI: {np.mean(bmi):.1f} ¬± {np.std(bmi):.1f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded. Please run preprocessing first.\")\n",
    "    print(\"This section will show comprehensive dataset statistics when data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d368d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. BLOOD PRESSURE DISTRIBUTION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None and sbp_labels is not None:\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Blood Pressure Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Histogram of SBP values\n",
    "    axes[0, 0].hist(sbp_labels, bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(np.mean(sbp_labels), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(sbp_labels):.1f} mmHg')\n",
    "    axes[0, 0].axvline(np.median(sbp_labels), color='orange', linestyle='--', \n",
    "                       label=f'Median: {np.median(sbp_labels):.1f} mmHg')\n",
    "    axes[0, 0].set_xlabel('Systolic Blood Pressure (mmHg)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('SBP Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot of SBP\n",
    "    bp = axes[0, 1].boxplot(sbp_labels, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    axes[0, 1].set_ylabel('Systolic Blood Pressure (mmHg)')\n",
    "    axes[0, 1].set_title('SBP Box Plot')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add outlier statistics\n",
    "    Q1 = np.percentile(sbp_labels, 25)\n",
    "    Q3 = np.percentile(sbp_labels, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = np.sum((sbp_labels < lower_bound) | (sbp_labels > upper_bound))\n",
    "    axes[0, 1].text(0.7, 0.95, f'Outliers: {outliers} ({100*outliers/len(sbp_labels):.1f}%)', \n",
    "                    transform=axes[0, 1].transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "    \n",
    "    # BP categories pie chart\n",
    "    bp_categories = ['Normal (<120)', 'Elevated (120-129)', 'Stage 1 (130-139)', 'Stage 2 (‚â•140)']\n",
    "    bp_counts = [\n",
    "        np.sum(sbp_labels < 120),\n",
    "        np.sum((sbp_labels >= 120) & (sbp_labels < 130)),\n",
    "        np.sum((sbp_labels >= 130) & (sbp_labels < 140)),\n",
    "        np.sum(sbp_labels >= 140)\n",
    "    ]\n",
    "    colors = ['lightgreen', 'yellow', 'orange', 'red']\n",
    "    \n",
    "    wedges, texts, autotexts = axes[1, 0].pie(bp_counts, labels=bp_categories, autopct='%1.1f%%', \n",
    "                                              colors=colors, startangle=90)\n",
    "    axes[1, 0].set_title('BP Categories Distribution (AHA Guidelines)')\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    from scipy import stats\n",
    "    stats.probplot(sbp_labels, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot: SBP Normality Check')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä STATISTICAL TESTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Normality test\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(sbp_labels[:5000] if len(sbp_labels) > 5000 else sbp_labels)\n",
    "    print(f\"Shapiro-Wilk normality test (p-value): {shapiro_p:.2e}\")\n",
    "    print(f\"Distribution is {'normal' if shapiro_p > 0.05 else 'not normal'} (Œ±=0.05)\")\n",
    "    \n",
    "    # Skewness and kurtosis\n",
    "    skewness = stats.skew(sbp_labels)\n",
    "    kurt = stats.kurtosis(sbp_labels)\n",
    "    print(f\"Skewness: {skewness:.3f} ({'right-skewed' if skewness > 0 else 'left-skewed' if skewness < 0 else 'symmetric'})\")\n",
    "    print(f\"Kurtosis: {kurt:.3f} ({'leptokurtic' if kurt > 0 else 'platykurtic' if kurt < 0 else 'mesokurtic'})\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded.\")\n",
    "    print(\"This section will create blood pressure distribution visualizations when data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data_quality():\n",
    "    \"\"\"Analyze data quality including missing values, outliers, and signal integrity.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç DATA QUALITY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Check for missing values in signals\n",
    "    missing_signals = np.sum(np.isnan(signals_sbp), axis=1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Data Quality Assessment', fontsize=16, y=0.98)\n",
    "    \n",
    "    # 1. Missing values distribution\n",
    "    axes[0, 0].hist(missing_signals, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Missing Values per Signal')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Missing Values in Signals')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add text annotation\n",
    "    total_missing = np.sum(missing_signals > 0)\n",
    "    axes[0, 0].text(0.7, 0.8, f'Signals with missing values: {total_missing}', \n",
    "                    transform=axes[0, 0].transAxes, \n",
    "                    bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "    \n",
    "    # 2. Signal amplitude outliers\n",
    "    signal_max = np.max(signals_sbp, axis=1)\n",
    "    signal_min = np.min(signals_sbp, axis=1)\n",
    "    \n",
    "    axes[0, 1].scatter(signal_min, signal_max, alpha=0.6, s=20)\n",
    "    axes[0, 1].set_xlabel('Signal Minimum')\n",
    "    axes[0, 1].set_ylabel('Signal Maximum')\n",
    "    axes[0, 1].set_title('Signal Amplitude Range')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Identify extreme outliers (beyond 3 standard deviations)\n",
    "    range_vals = signal_max - signal_min\n",
    "    outlier_threshold = np.mean(range_vals) + 3*np.std(range_vals)\n",
    "    outliers = range_vals > outlier_threshold\n",
    "    axes[0, 1].scatter(signal_min[outliers], signal_max[outliers], \n",
    "                      color='red', s=50, alpha=0.8, label=f'Outliers: {np.sum(outliers)}')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Signal length distribution\n",
    "    signal_lengths = [len(sig) for sig in signals_sbp if not np.all(np.isnan(sig))]\n",
    "    \n",
    "    axes[1, 0].hist(signal_lengths, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Signal Length (samples)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Signal Length Distribution')\n",
    "    axes[1, 0].axvline(np.mean(signal_lengths), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(signal_lengths):.0f}')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Resampling demonstration (if signals have different lengths)\n",
    "    demo_signal = signals_sbp[0][~np.isnan(signals_sbp[0])]\n",
    "    time_axis = np.arange(len(demo_signal)) / 125  # Assuming 125 Hz sampling\n",
    "    \n",
    "    target_length = int(np.median(signal_lengths))\n",
    "    if len(demo_signal) != target_length:\n",
    "        from scipy.signal import resample\n",
    "        resampled_signal = resample(demo_signal, target_length)\n",
    "        time_resampled = np.arange(target_length) / 125\n",
    "        \n",
    "        axes[1, 1].plot(time_axis, demo_signal, alpha=0.7, \n",
    "                       label=f'Original ({len(demo_signal)} samples)', linewidth=2)\n",
    "        axes[1, 1].plot(time_resampled, resampled_signal, alpha=0.8, \n",
    "                       label=f'Resampled ({target_length} samples)', linewidth=2)\n",
    "    else:\n",
    "        axes[1, 1].plot(time_axis, demo_signal, alpha=0.7, \n",
    "                       label='Signal (already target length)', linewidth=2)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Signal Amplitude')\n",
    "    axes[1, 1].set_title('Signal Resampling')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Quality Summary:\")\n",
    "    print(f\"Total signals: {len(signals_sbp)}\")\n",
    "    print(f\"Signals with missing values: {total_missing}\")\n",
    "    print(f\"Mean signal length: {np.mean(signal_lengths):.0f} samples\")\n",
    "    print(f\"Signal length std: {np.std(signal_lengths):.0f} samples\")\n",
    "    print(f\"Amplitude outliers: {np.sum(outliers)}\")\n",
    "    \n",
    "analyze_data_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. SIGNAL ANALYSIS & VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None:\n",
    "    from scipy.fft import fft, fftfreq\n",
    "    \n",
    "    # Select random samples for visualization\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    sample_indices = np.random.choice(len(signals_sbp), size=min(10, len(signals_sbp)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig.suptitle('Physiological Signal Analysis (PPG/ECG)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Signal examples (time domain)\n",
    "    time_axis = np.arange(signals_sbp.shape[1]) / 125  # Assuming 125 Hz sampling rate\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices[:5]):\n",
    "        color = plt.cm.tab10(i)\n",
    "        axes[0, 0].plot(time_axis, signals_sbp[idx], alpha=0.7, \n",
    "                       label=f'Sample {idx} (SBP: {sbp_labels[idx]:.0f})', color=color)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('Signal Amplitude')\n",
    "    axes[0, 0].set_title('Sample PPG/ECG Signals')\n",
    "    axes[0, 0].legend(loc='upper right', fontsize='small')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Signal statistics distribution\n",
    "    signal_means = np.mean(signals_sbp, axis=1)\n",
    "    signal_stds = np.std(signals_sbp, axis=1)\n",
    "    \n",
    "    axes[0, 1].hist(signal_means, bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Signal Mean')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Distribution of Signal Means')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Signal variance analysis\n",
    "    axes[1, 0].hist(signal_stds, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Signal Standard Deviation')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Distribution of Signal Variability')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Signal amplitude range\n",
    "    signal_ranges = np.max(signals_sbp, axis=1) - np.min(signals_sbp, axis=1)\n",
    "    axes[1, 1].hist(signal_ranges, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Signal Range (Max - Min)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Distribution of Signal Amplitude Ranges')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Frequency domain analysis (FFT of sample signals)\n",
    "    sample_signal = signals_sbp[sample_indices[0]]\n",
    "    fft_vals = np.abs(fft(sample_signal))\n",
    "    freqs = fftfreq(len(sample_signal), 1/125)  # 125 Hz sampling rate\n",
    "    \n",
    "    # Plot only positive frequencies up to Nyquist\n",
    "    pos_mask = freqs >= 0\n",
    "    axes[2, 0].plot(freqs[pos_mask][:len(freqs)//2], \n",
    "                   fft_vals[pos_mask][:len(freqs)//2])\n",
    "    axes[2, 0].set_xlabel('Frequency (Hz)')\n",
    "    axes[2, 0].set_ylabel('FFT Magnitude')\n",
    "    axes[2, 0].set_title('Frequency Domain Analysis (Sample Signal)')\n",
    "    axes[2, 0].set_xlim(0, 20)  # Focus on physiologically relevant frequencies\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Signal quality metrics\n",
    "    # Calculate SNR-like metric (signal power vs noise estimation)\n",
    "    signal_power = np.var(signals_sbp, axis=1)\n",
    "    noise_est = np.std(np.diff(signals_sbp, axis=1), axis=1)  # High-frequency noise estimate\n",
    "    snr_estimate = signal_power / (noise_est**2 + 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    axes[2, 1].hist(np.log10(snr_estimate), bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[2, 1].set_xlabel('Signal Quality (log10 SNR estimate)')\n",
    "    axes[2, 1].set_ylabel('Frequency')\n",
    "    axes[2, 1].set_title('Signal Quality Distribution')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Signal statistics summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä SIGNAL CHARACTERISTICS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Signal length: {signals_sbp.shape[1]} samples\")\n",
    "    print(f\"Estimated duration: {signals_sbp.shape[1]/125:.1f} seconds (@ 125 Hz)\")\n",
    "    print(f\"Signal amplitude range: {np.min(signals_sbp):.3f} to {np.max(signals_sbp):.3f}\")\n",
    "    print(f\"Mean signal amplitude: {np.mean(signal_means):.3f} ¬± {np.std(signal_means):.3f}\")\n",
    "    print(f\"Mean signal variability: {np.mean(signal_stds):.3f} ¬± {np.std(signal_stds):.3f}\")\n",
    "    print(f\"Average amplitude range: {np.mean(signal_ranges):.3f} ¬± {np.std(signal_ranges):.3f}\")\n",
    "    \n",
    "    # Correlation between signal characteristics and BP\n",
    "    corr_mean_bp = np.corrcoef(signal_means, sbp_labels)[0, 1]\n",
    "    corr_std_bp = np.corrcoef(signal_stds, sbp_labels)[0, 1]\n",
    "    corr_range_bp = np.corrcoef(signal_ranges, sbp_labels)[0, 1]\n",
    "    \n",
    "    print(f\"\\nSignal-BP Correlations:\")\n",
    "    print(f\"  Signal mean vs SBP: r = {corr_mean_bp:.3f}\")\n",
    "    print(f\"  Signal std vs SBP: r = {corr_std_bp:.3f}\")\n",
    "    print(f\"  Signal range vs SBP: r = {corr_range_bp:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No signal data loaded.\")\n",
    "    print(\"This section will analyze PPG/ECG signal characteristics when data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c14c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_comprehensive_data_quality():\n",
    "    \"\"\"Comprehensive data quality analysis including outlier detection and multivariate analysis.\"\"\"\n",
    "    if 'signals_sbp' in globals() and signals_sbp is not None:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç COMPREHENSIVE DATA QUALITY ASSESSMENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Prepare data\n",
    "        missing_signals = np.sum(np.isnan(signals_sbp), axis=1)\n",
    "        missing_demographics = np.sum(np.isnan(demographics_sbp), axis=1)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        fig.suptitle('Comprehensive Data Quality Assessment', fontsize=16, y=0.98)\n",
    "        \n",
    "        # 1. Missing values distribution\n",
    "        axes[0, 0].hist(missing_signals, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
    "        axes[0, 0].set_xlabel('Missing Values per Signal')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Missing Values in Signals')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add text annotation\n",
    "        total_missing = np.sum(missing_signals > 0)\n",
    "        axes[0, 0].text(0.7, 0.8, f'Signals with missing values: {total_missing}', \n",
    "                        transform=axes[0, 0].transAxes, \n",
    "                        bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "        \n",
    "        # 2. Signal amplitude outliers\n",
    "        signal_max = np.max(signals_sbp, axis=1)\n",
    "        signal_min = np.min(signals_sbp, axis=1)\n",
    "        \n",
    "        axes[0, 1].scatter(signal_min, signal_max, alpha=0.6, s=20)\n",
    "        axes[0, 1].set_xlabel('Signal Minimum')\n",
    "        axes[0, 1].set_ylabel('Signal Maximum')\n",
    "        axes[0, 1].set_title('Signal Amplitude Range')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Identify extreme outliers (beyond 3 standard deviations)\n",
    "        z_max = np.abs((signal_max - np.mean(signal_max)) / np.std(signal_max))\n",
    "        z_min = np.abs((signal_min - np.mean(signal_min)) / np.std(signal_min))\n",
    "        amplitude_outliers = (z_max > 3) | (z_min > 3)\n",
    "        \n",
    "        axes[0, 1].scatter(signal_min[amplitude_outliers], signal_max[amplitude_outliers], \n",
    "                          color='red', s=30, label=f'Outliers ({np.sum(amplitude_outliers)})')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # 3. Blood pressure outliers\n",
    "        # Using IQR method for BP outliers\n",
    "        Q1_bp = np.percentile(sbp_labels, 25)\n",
    "        Q3_bp = np.percentile(sbp_labels, 75)\n",
    "        IQR_bp = Q3_bp - Q1_bp\n",
    "        bp_outliers = (sbp_labels < (Q1_bp - 1.5 * IQR_bp)) | (sbp_labels > (Q3_bp + 1.5 * IQR_bp))\n",
    "        \n",
    "        axes[0, 2].hist(sbp_labels, bins=50, alpha=0.7, color='lightgreen', edgecolor='black', label='Normal')\n",
    "        axes[0, 2].hist(sbp_labels[bp_outliers], bins=20, alpha=0.8, color='red', edgecolor='black', label='Outliers')\n",
    "        axes[0, 2].axvline(Q1_bp - 1.5 * IQR_bp, color='red', linestyle='--', alpha=0.7, label='Lower bound')\n",
    "        axes[0, 2].axvline(Q3_bp + 1.5 * IQR_bp, color='red', linestyle='--', alpha=0.7, label='Upper bound')\n",
    "        axes[0, 2].set_xlabel('Systolic Blood Pressure (mmHg)')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].set_title(f'BP Outliers ({np.sum(bp_outliers)} samples)')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Multivariate outlier detection using Isolation Forest\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        # Combine signal features and demographics\n",
    "        features = np.column_stack([\n",
    "            np.mean(signals_sbp, axis=1),\n",
    "            np.std(signals_sbp, axis=1),\n",
    "            np.max(signals_sbp, axis=1) - np.min(signals_sbp, axis=1),\n",
    "            demographics_sbp[:, 0],  # Age\n",
    "            demographics_sbp[:, 2],  # Height\n",
    "            demographics_sbp[:, 3],  # Weight\n",
    "            sbp_labels\n",
    "        ])\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        # Apply Isolation Forest\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)  # Expect 10% outliers\n",
    "        outlier_labels = iso_forest.fit_predict(features_scaled)\n",
    "        outliers = outlier_labels == -1\n",
    "        \n",
    "        # Plot age vs SBP with outliers highlighted\n",
    "        age = demographics_sbp[:, 0]\n",
    "        axes[1, 0].scatter(age[~outliers], sbp_labels[~outliers], alpha=0.6, s=20, \n",
    "                          color='blue', label=f'Normal ({np.sum(~outliers)})')\n",
    "        axes[1, 0].scatter(age[outliers], sbp_labels[outliers], alpha=0.8, s=30, \n",
    "                          color='red', label=f'Outliers ({np.sum(outliers)})')\n",
    "        axes[1, 0].set_xlabel('Age (years)')\n",
    "        axes[1, 0].set_ylabel('SBP (mmHg)')\n",
    "        axes[1, 0].set_title('Multivariate Outliers (Age vs SBP)')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Signal noise analysis\n",
    "        # High-frequency noise estimation using signal derivatives\n",
    "        signal_noise = np.std(np.diff(signals_sbp, axis=1), axis=1)\n",
    "        \n",
    "        axes[1, 1].hist(signal_noise, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "        axes[1, 1].set_xlabel('Estimated Noise Level (Std of derivatives)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title('Signal Noise Distribution')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Identify noisy signals\n",
    "        noise_threshold = np.percentile(signal_noise, 95)  # Top 5% as noisy\n",
    "        noisy_signals = signal_noise > noise_threshold\n",
    "        axes[1, 1].axvline(noise_threshold, color='red', linestyle='--', \n",
    "                          label=f'95th percentile ({np.sum(noisy_signals)} signals)')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        # 6. Data completeness heatmap\n",
    "        # Create a sample of data completeness (for visualization)\n",
    "        sample_size = min(100, len(signals_sbp))\n",
    "        sample_indices = np.random.choice(len(signals_sbp), sample_size, replace=False)\n",
    "        \n",
    "        # Check for zeros or constant segments in signals (potential data quality issues)\n",
    "        completeness_matrix = np.zeros((sample_size, 4))  # 4 quality metrics\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            signal = signals_sbp[idx]\n",
    "            # Metric 1: Non-zero ratio\n",
    "            completeness_matrix[i, 0] = np.mean(signal != 0)\n",
    "            # Metric 2: Non-constant ratio (signal variability)\n",
    "            completeness_matrix[i, 1] = 1 - (np.sum(signal == signal[0]) / len(signal))\n",
    "            # Metric 3: Reasonable amplitude (not clipped)\n",
    "            completeness_matrix[i, 2] = 1 - ((np.sum(signal == np.max(signal)) + \n",
    "                                             np.sum(signal == np.min(signal))) / len(signal))\n",
    "            # Metric 4: Demographics completeness\n",
    "            completeness_matrix[i, 3] = 1 - (np.sum(np.isnan(demographics_sbp[idx])) / len(demographics_sbp[idx]))\n",
    "        \n",
    "        im = axes[1, 2].imshow(completeness_matrix.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "        axes[1, 2].set_xlabel('Sample Index')\n",
    "        axes[1, 2].set_ylabel('Quality Metrics')\n",
    "        axes[1, 2].set_title('Data Quality Heatmap (Sample)')\n",
    "        axes[1, 2].set_yticks([0, 1, 2, 3])\n",
    "        axes[1, 2].set_yticklabels(['Non-zero', 'Variable', 'Not-clipped', 'Demo-complete'])\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=axes[1, 2], shrink=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Data quality summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç DATA QUALITY ASSESSMENT SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Total samples: {len(signals_sbp):,}\")\n",
    "        print(f\"Signals with missing values: {np.sum(missing_signals > 0)}\")\n",
    "        print(f\"Demographics with missing values: {np.sum(missing_demographics > 0)}\")\n",
    "        \n",
    "        print(f\"\\nOutlier Detection:\")\n",
    "        print(f\"  Signal amplitude outliers: {np.sum(amplitude_outliers)} ({100*np.sum(amplitude_outliers)/len(signals_sbp):.1f}%)\")\n",
    "        print(f\"  Blood pressure outliers (IQR): {np.sum(bp_outliers)} ({100*np.sum(bp_outliers)/len(sbp_labels):.1f}%)\")\n",
    "        print(f\"  Multivariate outliers (Isolation Forest): {np.sum(outliers)} ({100*np.sum(outliers)/len(signals_sbp):.1f}%)\")\n",
    "        print(f\"  Noisy signals (top 5%): {np.sum(noisy_signals)} ({100*np.sum(noisy_signals)/len(signals_sbp):.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nSignal Quality Metrics:\")\n",
    "        print(f\"  Mean noise level: {np.mean(signal_noise):.4f} ¬± {np.std(signal_noise):.4f}\")\n",
    "        print(f\"  Average signal completeness: {np.mean(completeness_matrix):.3f}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\nüí° Data Quality Recommendations:\")\n",
    "        if np.sum(amplitude_outliers) > len(signals_sbp) * 0.05:\n",
    "            print(f\"  ‚ö†Ô∏è  High number of amplitude outliers - consider signal normalization\")\n",
    "        if np.sum(noisy_signals) > len(signals_sbp) * 0.1:\n",
    "            print(f\"  ‚ö†Ô∏è  Many noisy signals detected - apply filtering in preprocessing\")\n",
    "        if np.sum(bp_outliers) > len(sbp_labels) * 0.05:\n",
    "            print(f\"  ‚ö†Ô∏è  Significant BP outliers - verify measurement accuracy\")\n",
    "        if np.mean(completeness_matrix) < 0.95:\n",
    "            print(f\"  ‚ö†Ô∏è  Data completeness issues detected - investigate data collection\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No data loaded.\")\n",
    "        print(\"This section will assess data quality and detect outliers when data is available.\")\n",
    "\n",
    "analyze_comprehensive_data_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_engineering():\n",
    "    \"\"\"Explore potential features that can be extracted from the signals for model training.\"\"\"\n",
    "    if 'signals_sbp' in globals() and signals_sbp is not None:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üî¨ FEATURE ENGINEERING ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Extract various time-domain and frequency-domain features\n",
    "        n_samples = min(100, len(signals_sbp))\n",
    "        feature_matrix = np.zeros((n_samples, 10))  # 10 features\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            signal = signals_sbp[i]\n",
    "            if not np.all(np.isnan(signal)):\n",
    "                # Time-domain features\n",
    "                feature_matrix[i, 0] = np.mean(signal)           # Mean\n",
    "                feature_matrix[i, 1] = np.std(signal)            # Standard deviation\n",
    "                feature_matrix[i, 2] = np.max(signal) - np.min(signal)  # Range\n",
    "                feature_matrix[i, 3] = np.var(signal)            # Variance\n",
    "                \n",
    "                # Pulse-related features\n",
    "                peaks, _ = find_peaks(signal, height=np.mean(signal), distance=20)\n",
    "                feature_matrix[i, 4] = len(peaks)                # Peak count\n",
    "                if len(peaks) > 1:\n",
    "                    feature_matrix[i, 5] = np.mean(np.diff(peaks))  # Mean peak interval\n",
    "                else:\n",
    "                    feature_matrix[i, 5] = 0\n",
    "                \n",
    "                # Frequency-domain features\n",
    "                fft_signal = np.fft.fft(signal)\n",
    "                power_spectrum = np.abs(fft_signal)**2\n",
    "                feature_matrix[i, 6] = np.argmax(power_spectrum[1:len(power_spectrum)//2]) + 1  # Dominant frequency index\n",
    "                feature_matrix[i, 7] = np.sum(power_spectrum)    # Total power\n",
    "                \n",
    "                # Statistical features\n",
    "                feature_matrix[i, 8] = np.percentile(signal, 75) - np.percentile(signal, 25)  # IQR\n",
    "                feature_matrix[i, 9] = len(signal)               # Signal length\n",
    "        \n",
    "        # Create visualizations\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "        fig.suptitle('Feature Engineering Analysis', fontsize=16, y=0.98)\n",
    "        \n",
    "        feature_names = ['Mean', 'Std Dev', 'Range', 'Variance', 'Peak Count', \n",
    "                        'Mean Peak Interval', 'Dominant Freq Index', 'Total Power', 'IQR']\n",
    "        \n",
    "        # Plot feature distributions\n",
    "        for i, (feature_name, ax) in enumerate(zip(feature_names, axes.flat)):\n",
    "            if i < len(feature_names):\n",
    "                ax.hist(feature_matrix[:, i], bins=20, alpha=0.7, edgecolor='black')\n",
    "                ax.set_xlabel(feature_name)\n",
    "                ax.set_ylabel('Frequency')\n",
    "                ax.set_title(f'{feature_name} Distribution')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Feature correlation with blood pressure\n",
    "        correlations = []\n",
    "        sample_bp = sbp_labels[:n_samples]\n",
    "        \n",
    "        for i in range(feature_matrix.shape[1]):\n",
    "            corr = np.corrcoef(feature_matrix[:, i], sample_bp)[0, 1]\n",
    "            correlations.append(corr)\n",
    "        \n",
    "        # Bar plot of correlations\n",
    "        axes[2, 2].barh(range(len(feature_names)), correlations, alpha=0.7)\n",
    "        axes[2, 2].set_yticks(range(len(feature_names)))\n",
    "        axes[2, 2].set_yticklabels(feature_names)\n",
    "        axes[2, 2].set_xlabel('Correlation with SBP')\n",
    "        axes[2, 2].set_title('Feature-SBP Correlations')\n",
    "        axes[2, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation values as text\n",
    "        for i, corr in enumerate(correlations):\n",
    "            axes[2, 2].text(corr + 0.01, i, f'{corr:.3f}', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance summary\n",
    "        print(f\"\\n\udcca Feature Analysis Summary:\")\n",
    "        print(f\"Features extracted from {n_samples} signals\")\n",
    "        print(f\"\\nTop correlated features with SBP:\")\n",
    "        \n",
    "        # Sort features by correlation magnitude\n",
    "        feature_corr_pairs = list(zip(feature_names, correlations))\n",
    "        feature_corr_pairs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        for i, (feature_name, corr) in enumerate(feature_corr_pairs[:5]):\n",
    "            print(f\"  {i+1}. {feature_name}: r = {corr:.3f}\")\n",
    "        \n",
    "        print(f\"\\nRecommended features for model training:\")\n",
    "        strong_features = [name for name, corr in feature_corr_pairs if abs(corr) > 0.1]\n",
    "        for feature in strong_features:\n",
    "            print(f\"  ‚Ä¢ {feature}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No data loaded.\")\n",
    "        print(\"This section will analyze potential features when data is available.\")\n",
    "\n",
    "# Import required functions for feature extraction\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "analyze_feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b48ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. FEATURE ENGINEERING & PREPROCESSING INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "if signals_sbp is not None:\n",
    "    from scipy.signal import butter, filtfilt, resample\n",
    "    from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "    from scipy.fft import fft, fftfreq\n",
    "    from scipy import stats\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig.suptitle('Feature Engineering & Preprocessing Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Select a representative signal for preprocessing demonstration\n",
    "    demo_signal = signals_sbp[sample_indices[0]].copy()\n",
    "    original_signal = demo_signal.copy()\n",
    "    \n",
    "    # 1. Signal filtering effects\n",
    "    # Design a bandpass filter (typical for PPG: 0.5-8 Hz)\n",
    "    nyquist = 125 / 2  # Half of sampling frequency\n",
    "    low_cut = 0.5 / nyquist\n",
    "    high_cut = 8.0 / nyquist\n",
    "    b, a = butter(4, [low_cut, high_cut], btype='band')\n",
    "    filtered_signal = filtfilt(b, a, demo_signal)\n",
    "    \n",
    "    axes[0, 0].plot(time_axis, original_signal, alpha=0.7, label='Original', linewidth=2)\n",
    "    axes[0, 0].plot(time_axis, filtered_signal, alpha=0.8, label='Filtered (0.5-8 Hz)', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('Signal Amplitude')\n",
    "    axes[0, 0].set_title('Effect of Bandpass Filtering')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Normalization comparison\n",
    "    # Min-Max normalization\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    signal_minmax = minmax_scaler.fit_transform(demo_signal.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Z-score normalization\n",
    "    signal_zscore = (demo_signal - np.mean(demo_signal)) / np.std(demo_signal)\n",
    "    \n",
    "    # Robust normalization (median-based)\n",
    "    robust_scaler = RobustScaler()\n",
    "    signal_robust = robust_scaler.fit_transform(demo_signal.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    axes[0, 1].plot(time_axis, signal_minmax, alpha=0.8, label='Min-Max [0,1]', linewidth=2)\n",
    "    axes[0, 1].plot(time_axis, signal_zscore, alpha=0.8, label='Z-score', linewidth=2)\n",
    "    axes[0, 1].plot(time_axis, signal_robust, alpha=0.8, label='Robust (IQR)', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Time (seconds)')\n",
    "    axes[0, 1].set_ylabel('Normalized Amplitude')\n",
    "    axes[0, 1].set_title('Normalization Methods Comparison')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Signal length distribution and resampling analysis\n",
    "    signal_lengths = [len(sig) for sig in signals_sbp]\n",
    "    axes[1, 0].hist(signal_lengths, bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Signal Length (samples)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Signal Length Distribution')\n",
    "    axes[1, 0].axvline(np.mean(signal_lengths), color='red', linestyle='--', \n",
    "                      label=f'Mean: {np.mean(signal_lengths):.0f}')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Resampling demonstration (if signals have different lengths)\n",
    "    target_length = int(np.median(signal_lengths))\n",
    "    if len(demo_signal) != target_length:\n",
    "        resampled_signal = resample(demo_signal, target_length)\n",
    "        time_resampled = np.arange(target_length) / 125\n",
    "        \n",
    "        axes[1, 1].plot(time_axis, demo_signal, alpha=0.7, label=f'Original ({len(demo_signal)} samples)', linewidth=2)\n",
    "        axes[1, 1].plot(time_resampled, resampled_signal, alpha=0.8, \n",
    "                       label=f'Resampled ({target_length} samples)', linewidth=2)\n",
    "    else:\n",
    "        axes[1, 1].plot(time_axis, demo_signal, alpha=0.7, label='Signal (already target length)', linewidth=2)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Signal Amplitude')\n",
    "    axes[1, 1].set_title('Signal Resampling')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Feature extraction demonstration\n",
    "    # Extract various time-domain and frequency-domain features\n",
    "    n_samples = min(100, len(signals_sbp))\n",
    "    feature_matrix = np.zeros((n_samples, 10))  # 10 features\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        signal = signals_sbp[i]\n",
    "        \n",
    "        # Time domain features\n",
    "        feature_matrix[i, 0] = np.mean(signal)  # Mean\n",
    "        feature_matrix[i, 1] = np.std(signal)   # Standard deviation\n",
    "        feature_matrix[i, 2] = np.max(signal) - np.min(signal)  # Range\n",
    "        feature_matrix[i, 3] = np.sqrt(np.mean(signal**2))  # RMS\n",
    "        feature_matrix[i, 4] = stats.skew(signal)  # Skewness\n",
    "        \n",
    "        # Frequency domain features (simple spectral analysis)\n",
    "        fft_vals = np.abs(fft(signal))\n",
    "        freqs = fftfreq(len(signal), 1/125)\n",
    "        pos_freqs = freqs[:len(freqs)//2]\n",
    "        pos_fft = fft_vals[:len(fft_vals)//2]\n",
    "        \n",
    "        # Spectral centroid (weighted mean frequency)\n",
    "        feature_matrix[i, 5] = np.sum(pos_freqs * pos_fft) / np.sum(pos_fft) if np.sum(pos_fft) > 0 else 0\n",
    "        \n",
    "        # Spectral energy in different bands\n",
    "        low_band = (pos_freqs >= 0.5) & (pos_freqs < 2.0)\n",
    "        mid_band = (pos_freqs >= 2.0) & (pos_freqs < 5.0)\n",
    "        high_band = (pos_freqs >= 5.0) & (pos_freqs < 8.0)\n",
    "        \n",
    "        feature_matrix[i, 6] = np.sum(pos_fft[low_band])   # Low frequency energy\n",
    "        feature_matrix[i, 7] = np.sum(pos_fft[mid_band])   # Mid frequency energy\n",
    "        feature_matrix[i, 8] = np.sum(pos_fft[high_band])  # High frequency energy\n",
    "        \n",
    "        # Signal complexity (approximate entropy)\n",
    "        feature_matrix[i, 9] = np.std(np.diff(signal))  # First difference std as complexity measure\n",
    "    \n",
    "    # Plot feature correlation with SBP\n",
    "    feature_names = ['Mean', 'Std', 'Range', 'RMS', 'Skewness', \n",
    "                    'Spec.Centroid', 'Low Freq', 'Mid Freq', 'High Freq', 'Complexity']\n",
    "    \n",
    "    correlations = []\n",
    "    for j in range(feature_matrix.shape[1]):\n",
    "        corr = np.corrcoef(feature_matrix[:, j], sbp_labels[:n_samples])[0, 1]\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    bars = axes[2, 0].bar(range(len(correlations)), correlations, \n",
    "                          color=['red' if abs(c) > 0.1 else 'lightblue' for c in correlations])\n",
    "    axes[2, 0].set_xlabel('Features')\n",
    "    axes[2, 0].set_ylabel('Correlation with SBP')\n",
    "    axes[2, 0].set_title('Feature-SBP Correlations')\n",
    "    axes[2, 0].set_xticks(range(len(feature_names)))\n",
    "    axes[2, 0].set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    axes[2, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add correlation values on bars\n",
    "    for bar, corr in zip(bars, correlations):\n",
    "        height = bar.get_height()\n",
    "        axes[2, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{corr:.3f}', ha='center', va='bottom' if corr > 0 else 'top')\n",
    "    \n",
    "    # 6. Feature distribution comparison\n",
    "    # Show distribution of most correlated feature\n",
    "    best_feature_idx = np.argmax(np.abs(correlations))\n",
    "    best_feature = feature_matrix[:, best_feature_idx]\n",
    "    \n",
    "    axes[2, 1].scatter(best_feature, sbp_labels[:n_samples], alpha=0.6)\n",
    "    axes[2, 1].set_xlabel(f'{feature_names[best_feature_idx]}')\n",
    "    axes[2, 1].set_ylabel('SBP (mmHg)')\n",
    "    axes[2, 1].set_title(f'Best Feature vs SBP (r = {correlations[best_feature_idx]:.3f})')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(best_feature, sbp_labels[:n_samples], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[2, 1].plot(best_feature, p(best_feature), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature engineering summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîß FEATURE ENGINEERING & PREPROCESSING INSIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"Signal characteristics:\")\n",
    "    print(f\"  Original signal range: [{np.min(original_signal):.3f}, {np.max(original_signal):.3f}]\")\n",
    "    print(f\"  After filtering: [{np.min(filtered_signal):.3f}, {np.max(filtered_signal):.3f}]\")\n",
    "    print(f\"  Signal length statistics: {np.mean(signal_lengths):.0f} ¬± {np.std(signal_lengths):.0f} samples\")\n",
    "    \n",
    "    print(f\"\\nFeature correlations with SBP:\")\n",
    "    for i, (name, corr) in enumerate(zip(feature_names, correlations)):\n",
    "        significance = \"***\" if abs(corr) > 0.3 else \"**\" if abs(corr) > 0.2 else \"*\" if abs(corr) > 0.1 else \"\"\n",
    "        print(f\"  {name:15s}: {corr:6.3f} {significance}\")\n",
    "    \n",
    "    print(f\"\\nPreprocessing recommendations:\")\n",
    "    print(f\"  üîÑ Apply bandpass filtering (0.5-8 Hz) to remove noise and artifacts\")\n",
    "    if np.std(signal_lengths) > np.mean(signal_lengths) * 0.1:\n",
    "        print(f\"  üìè Standardize signal length to {int(np.median(signal_lengths))} samples\")\n",
    "    print(f\"  üìä Use robust normalization to handle outliers\")\n",
    "    print(f\"  üéØ Focus on features with |correlation| > 0.1 for model training\")\n",
    "    \n",
    "    # Identify best features\n",
    "    good_features = [feature_names[i] for i, c in enumerate(correlations) if abs(c) > 0.1]\n",
    "    if good_features:\n",
    "        print(f\"  ‚≠ê Promising features: {', '.join(good_features)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data loaded.\")\n",
    "    print(\"This section will analyze feature engineering and preprocessing when data is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed7956",
   "metadata": {},
   "source": [
    "## üìã Summary & Conclusions\n",
    "\n",
    "This comprehensive EDA notebook provides insights into the cuffless blood pressure dataset for model development:\n",
    "\n",
    "### Key Findings:\n",
    "- **Dataset Overview**: Complete statistics on signal characteristics, BP distributions, and demographic patterns\n",
    "- **Signal Quality**: Assessment of noise levels, outliers, and data completeness\n",
    "- **Feature Correlations**: Identification of features most correlated with blood pressure\n",
    "- **Preprocessing Insights**: Optimal filtering, normalization, and feature engineering strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
