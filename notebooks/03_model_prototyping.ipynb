{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21499db",
   "metadata": {},
   "source": [
    "# Cuffless Blood Pressure Estimation - Model Prototyping\n",
    "\n",
    "This notebook implements and evaluates various deep learning models for cuffless blood pressure estimation using PPG/ECG signals.\n",
    "\n",
    "## üéØ Model Architectures\n",
    "\n",
    "1. **CNN-Based Models**\n",
    "   - 1D CNN for feature extraction from physiological signals\n",
    "   - Convolutional layers to capture temporal patterns\n",
    "   - Feature maps for signal morphology analysis\n",
    "\n",
    "2. **LSTM-Based Models**  \n",
    "   - Long Short-Term Memory for temporal dependencies\n",
    "   - Bidirectional LSTM for forward/backward signal analysis\n",
    "   - Sequence-to-value regression\n",
    "\n",
    "3. **Hybrid CNN-LSTM Models**\n",
    "   - CNN for local feature extraction + LSTM for temporal modeling\n",
    "   - Optimal combination for physiological signal processing\n",
    "   - State-of-the-art performance for BP estimation\n",
    "\n",
    "## üìä Evaluation Metrics\n",
    "- **Mean Absolute Error (MAE)**\n",
    "- **Root Mean Square Error (RMSE)**\n",
    "- **Mean Absolute Percentage Error (MAPE)**\n",
    "- **Correlation Coefficient (R)**\n",
    "- **Standard Deviation (STD)**\n",
    "\n",
    "## üî¨ Experimental Setup\n",
    "- Train/Validation/Test split: 70/15/15\n",
    "- Cross-validation for robust evaluation\n",
    "- Hyperparameter optimization\n",
    "- Model comparison and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909151fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning framework\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, LSTM, Conv1D, MaxPooling1D, GlobalAveragePooling1D,\n",
    "    Bidirectional, Dropout, BatchNormalization, Flatten,\n",
    "    TimeDistributed, Input, Concatenate\n",
    ")\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Signal processing\n",
    "from scipy import stats\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# TensorFlow configuration\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU'))} devices\")\n",
    "\n",
    "# Plot configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. DATA LOADING (MAT/HDF5) + AGGREGATION\n",
    "# =============================================================================\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def _read_subj_wins(path, field='Subj_Wins', sample_limit=None):\n",
    "    \"\"\"Read per-window data from a MAT v7.3 'Subj_Wins' group without dereferencing the whole file.\n",
    "\n",
    "    Extracts numeric Age, Gender, Height, and Weight per-window when available. Missing values\n",
    "    are returned as np.nan (not 0) to avoid masking missingness.\n",
    "    \"\"\"\n",
    "    signals = []\n",
    "    sbps = []\n",
    "    dbps = []\n",
    "    ages = []\n",
    "    genders = []\n",
    "    heights = []\n",
    "    weights = []\n",
    "    with h5py.File(path, 'r') as f2:\n",
    "        if field not in f2:\n",
    "            raise KeyError(f'{field} not found in {path}')\n",
    "        sw = f2[field]\n",
    "\n",
    "        # Get signal references\n",
    "        ppg_refs = sw.get('PPG_Raw')\n",
    "\n",
    "        # Try common keys for ECG signal\n",
    "        ecg_key_found = None\n",
    "        for key in ['ECG_Raw', 'ECG_Raw_II', 'II', 'ECG', 'ecg']:\n",
    "            if key in sw:\n",
    "                ecg_key_found = key\n",
    "                break\n",
    "\n",
    "        if ppg_refs is None or ecg_key_found is None:\n",
    "            raise KeyError(f'Could not find PPG_Raw or a valid ECG signal key in {list(sw.keys())}')\n",
    "\n",
    "        ecg_refs = sw.get(ecg_key_found)\n",
    "\n",
    "        n = ppg_refs.shape[1] if getattr(ppg_refs, 'ndim', 0) > 1 else ppg_refs.shape[0]\n",
    "        if sample_limit is not None:\n",
    "            n = min(int(n), int(sample_limit))\n",
    "\n",
    "        for i in range(int(n)):\n",
    "            # Dereference PPG signal\n",
    "            ppg_ref = ppg_refs[0, i] if getattr(ppg_refs, 'ndim', 0) == 2 and ppg_refs.shape[0] == 1 else ppg_refs[i]\n",
    "            try:\n",
    "                ppg_sig = f2[ppg_ref][()] if isinstance(ppg_ref, h5py.Reference) else ppg_ref\n",
    "            except Exception:\n",
    "                ppg_sig = ppg_ref\n",
    "\n",
    "            # Dereference ECG signal\n",
    "            ecg_ref = ecg_refs[0, i] if getattr(ecg_refs, 'ndim', 0) == 2 and ecg_refs.shape[0] == 1 else ecg_refs[i]\n",
    "            try:\n",
    "                ecg_sig = f2[ecg_ref][()] if isinstance(ecg_ref, h5py.Reference) else ecg_ref\n",
    "            except Exception:\n",
    "                ecg_sig = ecg_ref\n",
    "\n",
    "            # Combine signals into a (2, L) array\n",
    "            sig = np.vstack([np.asarray(ppg_sig).ravel(), np.asarray(ecg_sig).ravel()])\n",
    "            signals.append(sig)\n",
    "\n",
    "            def _get_field_val(name):\n",
    "                d = sw.get(name)\n",
    "                if d is None:\n",
    "                    return np.nan\n",
    "                ref2 = d[0, i] if getattr(d, 'ndim', 0) == 2 and d.shape[0] == 1 else d[i]\n",
    "                try:\n",
    "                    val = f2[ref2][()]\n",
    "                except Exception:\n",
    "                    val = ref2\n",
    "                arr = np.asarray(val)\n",
    "                if arr.size == 1:\n",
    "                    try:\n",
    "                        return float(arr.item())\n",
    "                    except Exception:\n",
    "                        return np.nan\n",
    "                return np.nan\n",
    "\n",
    "            sbps.append(_get_field_val('SegSBP'))\n",
    "            dbps.append(_get_field_val('SegDBP'))\n",
    "            ages.append(_get_field_val('Age'))\n",
    "            genders.append(_get_field_val('Gender'))\n",
    "            heights.append(_get_field_val('Height'))\n",
    "            weights.append(_get_field_val('Weight'))\n",
    "\n",
    "    sigs = np.array(signals, dtype=object) if any(s.shape != signals[0].shape for s in signals) else np.stack(signals)\n",
    "\n",
    "    sbp_arr = np.array(sbps, dtype=float)\n",
    "    dbp_arr = np.array(dbps, dtype=float)\n",
    "    age_arr = np.array(ages, dtype=float)\n",
    "\n",
    "    def _gdecode(v):\n",
    "        try:\n",
    "            if v is None:\n",
    "                return np.nan\n",
    "            if isinstance(v, (bytes, bytearray)):\n",
    "                s = v.decode(errors='ignore').strip().upper()\n",
    "            elif isinstance(v, str):\n",
    "                s = v.strip().upper()\n",
    "            else:\n",
    "                iv = float(v)\n",
    "                if iv in (1.0, 0.0):\n",
    "                    return iv\n",
    "                if int(iv) in (77, 70):\n",
    "                    s = chr(int(iv))\n",
    "                else:\n",
    "                    return np.nan\n",
    "            return 1.0 if s == 'M' else 0.0 if s == 'F' else np.nan\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    gender_arr = np.array([_gdecode(g) for g in genders], dtype=float)\n",
    "    height_arr = np.array(heights, dtype=float)\n",
    "    weight_arr = np.array(weights, dtype=float)\n",
    "\n",
    "    demographics = np.column_stack([age_arr, gender_arr, height_arr, weight_arr])\n",
    "    return sigs, sbp_arr, demographics\n",
    "\n",
    "\n",
    "def load_aggregate_data():\n",
    "    \"\"\"Load and aggregate data from all patient .mat files.\"\"\"\n",
    "    processed_dir = Path('../data/processed')\n",
    "    if not processed_dir.exists():\n",
    "        print(\"‚ùå Processed data directory not found.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    mat_files = sorted([f for f in os.listdir(processed_dir) if f.endswith('.mat')])\n",
    "    if not mat_files:\n",
    "        print(\"‚ùå No .mat files found in the processed data directory.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    all_signals, all_labels, all_demographics, all_patient_ids = [], [], [], []\n",
    "\n",
    "    print(f\"üîÑ Loading data from {len(mat_files)} patient files...\")\n",
    "    for file_name in mat_files:\n",
    "        patient_id = file_name.split('.')[0]\n",
    "        file_path = str(processed_dir / file_name)\n",
    "\n",
    "        signals, sbp, demographics = None, None, None\n",
    "\n",
    "        try:\n",
    "            # First, try to load with scipy.io.loadmat for older MAT files\n",
    "            data = loadmat(file_path, squeeze_me=True, struct_as_record=False)\n",
    "\n",
    "            if 'Subj_Wins' in data:\n",
    "                subset = data['Subj_Wins']\n",
    "                ppg = subset.PPG_Raw\n",
    "                ecg = subset.ECG_Raw\n",
    "                # Ensure signals are correctly stacked\n",
    "                signals = np.array([np.vstack([p.ravel(), e.ravel()]) for p, e in zip(ppg, ecg)])\n",
    "                sbp = subset.SegSBP\n",
    "\n",
    "                age = subset.Age\n",
    "                gender = subset.Gender\n",
    "                height = subset.Height\n",
    "                weight = subset.Weight\n",
    "                demographics = np.column_stack([age, gender, height, weight])\n",
    "            else:\n",
    "                print(f\"   ‚ùå Could not find 'Subj_Wins' structure in {file_name} using scipy. Skipping file.\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            # If scipy fails, check if it's an HDF5 file and use the h5py reader\n",
    "            msg = str(e).lower()\n",
    "            if 'hdf' in msg or '7.3' in msg or 'h5py' in msg:\n",
    "                try:\n",
    "                    signals, sbp, demographics = _read_subj_wins(file_path)\n",
    "                except Exception as h5_e:\n",
    "                    print(f\"   ‚ùå Failed to load {file_name} with HDF5 reader. Error: {h5_e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"   ‚ùå Failed to load {file_name} with scipy.io.loadmat. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        if signals is not None and len(signals) > 0:\n",
    "            all_signals.append(signals)\n",
    "            all_labels.append(sbp)\n",
    "            all_demographics.append(demographics)\n",
    "            all_patient_ids.extend([patient_id] * len(signals))\n",
    "\n",
    "    if not all_signals:\n",
    "        print(\"‚ùå No data could be loaded from any files.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Concatenate all data\n",
    "    signals_agg = np.vstack(all_signals)\n",
    "    labels_agg = np.concatenate(all_labels)\n",
    "    demographics_agg = np.vstack(all_demographics)\n",
    "    patient_ids_agg = np.array(all_patient_ids)\n",
    "\n",
    "    # Filter out rows with NaN labels\n",
    "    valid_indices = ~np.isnan(labels_agg)\n",
    "    signals_agg = signals_agg[valid_indices]\n",
    "    labels_agg = labels_agg[valid_indices]\n",
    "    demographics_agg = demographics_agg[valid_indices]\n",
    "    patient_ids_agg = patient_ids_agg[valid_indices]\n",
    "\n",
    "    print(f\"‚úÖ Aggregated data loaded successfully!\")\n",
    "    print(f\"   - Total signals: {signals_agg.shape}\")\n",
    "    print(f\"   - Total labels: {labels_agg.shape}\")\n",
    "    print(f\"   - Total demographics: {demographics_agg.shape}\")\n",
    "    print(f\"   - Total patient IDs: {patient_ids_agg.shape}\")\n",
    "    print(f\"   - Unique patients: {len(np.unique(patient_ids_agg))}\")\n",
    "\n",
    "    return signals_agg, labels_agg, demographics_agg, patient_ids_agg\n",
    "\n",
    "\n",
    "# --- Workflow Step 1: Load aggregated data ---\n",
    "signals_agg, labels_agg, demographics_agg, patient_ids_agg = load_aggregate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55515a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. SIGNAL PREPROCESSING (PPG/ECG)\n",
    "# =============================================================================\n",
    "def preprocess_signals(signals, target_length=875, sampling_rate=125):\n",
    "    \"\"\"Preprocess physiological signals for model input.\"\"\"\n",
    "    print(\"üîÑ Preprocessing signals...\")\n",
    "\n",
    "    processed_signals_all = []\n",
    "    for signal_window in signals:\n",
    "        # Assuming channel 0 is PPG, channel 1 is ECG\n",
    "        ppg_signal = signal_window[0, :]\n",
    "        ecg_signal = signal_window[1, :]\n",
    "\n",
    "        # PPG bandpass filtering (0.5-8 Hz)\n",
    "        nyquist_ppg = sampling_rate / 2\n",
    "        low_cut_ppg = 0.5 / nyquist_ppg\n",
    "        high_cut_ppg = 8.0 / nyquist_ppg\n",
    "        b_ppg, a_ppg = butter(4, [low_cut_ppg, high_cut_ppg], btype='band')\n",
    "        filtered_ppg = filtfilt(b_ppg, a_ppg, ppg_signal)\n",
    "\n",
    "        # ECG bandpass filtering (0.5-40 Hz)\n",
    "        nyquist_ecg = sampling_rate / 2\n",
    "        low_cut_ecg = 0.5 / nyquist_ecg\n",
    "        high_cut_ecg = 40.0 / nyquist_ecg\n",
    "        b_ecg, a_ecg = butter(4, [low_cut_ecg, high_cut_ecg], btype='band')\n",
    "        filtered_ecg = filtfilt(b_ecg, a_ecg, ecg_signal)\n",
    "\n",
    "        # Standardize length by truncating or padding\n",
    "        def standardize_length(signal, length):\n",
    "            if len(signal) > length:\n",
    "                start_idx = (len(signal) - length) // 2\n",
    "                return signal[start_idx:start_idx + length]\n",
    "            padding = length - len(signal)\n",
    "            pad_left = padding // 2\n",
    "            pad_right = padding - pad_left\n",
    "            return np.pad(signal, (pad_left, pad_right), mode='constant')\n",
    "\n",
    "        processed_ppg = standardize_length(filtered_ppg, target_length)\n",
    "        processed_ecg = standardize_length(filtered_ecg, target_length)\n",
    "\n",
    "        processed_signals_all.append(np.stack([processed_ppg, processed_ecg], axis=0))\n",
    "\n",
    "    processed_signals = np.array(processed_signals_all)\n",
    "\n",
    "    # Normalization (Z-score per channel)\n",
    "    mean = np.mean(processed_signals, axis=2, keepdims=True)\n",
    "    std = np.std(processed_signals, axis=2, keepdims=True)\n",
    "    processed_signals = (processed_signals - mean) / (std + 1e-8)\n",
    "\n",
    "    # Transpose to (samples, timesteps, channels) for Keras\n",
    "    processed_signals = np.transpose(processed_signals, (0, 2, 1))\n",
    "\n",
    "    print(f\"   - Signal preprocessing complete: {processed_signals.shape}\")\n",
    "    print(f\"   - Target length: {target_length} samples ({target_length/sampling_rate:.1f} seconds)\")\n",
    "    return processed_signals\n",
    "\n",
    "\n",
    "if signals_agg is not None:\n",
    "    processed_signals = preprocess_signals(signals_agg)\n",
    "    y = labels_agg\n",
    "else:\n",
    "    processed_signals = None\n",
    "    y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4312ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. SUBJECT-WISE SPLIT + DEMOGRAPHICS PREP\n",
    "# =============================================================================\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def create_subject_wise_splits(patient_ids, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"Creates train/validation/test splits that are subject-wise and returns boolean masks.\"\"\"\n",
    "    print(\"üìä Creating subject-wise data splits...\")\n",
    "\n",
    "    unique_patients = np.unique(patient_ids)\n",
    "    n_patients = len(unique_patients)\n",
    "\n",
    "    # Split patient IDs into train, val, test\n",
    "    train_val_pids, test_pids = train_test_split(unique_patients, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Adjust val_size relative to the remaining pool of patients\n",
    "    relative_val_size = val_size / (1 - test_size)\n",
    "    train_pids, val_pids = train_test_split(train_val_pids, test_size=relative_val_size, random_state=42)\n",
    "\n",
    "    print(f\"   - Total unique patients: {n_patients}\")\n",
    "    print(f\"   - Train patients: {len(train_pids)}\")\n",
    "    print(f\"   - Validation patients: {len(val_pids)}\")\n",
    "    print(f\"   - Test patients: {len(test_pids)}\")\n",
    "\n",
    "    train_mask = np.isin(patient_ids, train_pids)\n",
    "    val_mask = np.isin(patient_ids, val_pids)\n",
    "    test_mask = np.isin(patient_ids, test_pids)\n",
    "\n",
    "    print(f\"\\n   - Train samples: {np.sum(train_mask)} ({np.sum(train_mask)/len(patient_ids)*100:.1f}%)\")\n",
    "    print(f\"   - Validation samples: {np.sum(val_mask)} ({np.sum(val_mask)/len(patient_ids)*100:.1f}%)\")\n",
    "    print(f\"   - Test samples: {np.sum(test_mask)} ({np.sum(test_mask)/len(patient_ids)*100:.1f}%)\")\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "if signals_agg is not None:\n",
    "    train_mask, val_mask, test_mask = create_subject_wise_splits(patient_ids_agg)\n",
    "\n",
    "    print(\"\\nüîÑ Imputing and scaling demographic data...\")\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    scaler_demo = StandardScaler()\n",
    "\n",
    "    demo_train_raw = demographics_agg[train_mask]\n",
    "    demo_val_raw = demographics_agg[val_mask]\n",
    "    demo_test_raw = demographics_agg[test_mask]\n",
    "\n",
    "    demo_train = imputer.fit_transform(demo_train_raw)\n",
    "    demo_train = scaler_demo.fit_transform(demo_train)\n",
    "\n",
    "    demo_val = imputer.transform(demo_val_raw)\n",
    "    demo_val = scaler_demo.transform(demo_val)\n",
    "\n",
    "    demo_test = imputer.transform(demo_test_raw)\n",
    "    demo_test = scaler_demo.transform(demo_test)\n",
    "\n",
    "    print(\"   - Missing values and scaling handled for demographics.\")\n",
    "else:\n",
    "    train_mask = val_mask = test_mask = None\n",
    "    demo_train = demo_val = demo_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. PHYSIOLOGICAL FEATURE EXTRACTION (PAT, HR)\n",
    "# =============================================================================\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def extract_physiological_features(ecg_signals, ppg_signals, sampling_rate=125):\n",
    "    \"\"\"Extract Pulse Arrival Time (PAT) and Heart Rate (HR) from ECG and PPG signals.\"\"\"\n",
    "    print(\"üíì Extracting physiological features (PAT, HR)...\")\n",
    "\n",
    "    all_pat_sequences = []\n",
    "    all_hr_sequences = []\n",
    "    all_peak_indices = {'r_peaks': [], 'ppg_feet': []}\n",
    "\n",
    "    for i in range(ecg_signals.shape[0]):\n",
    "        ecg = ecg_signals[i]\n",
    "        ppg = ppg_signals[i]\n",
    "\n",
    "        # --- R-peak detection for HR ---\n",
    "        r_peaks, _ = find_peaks(ecg, height=np.mean(ecg) + 1.5 * np.std(ecg), distance=sampling_rate * 60/80)\n",
    "        all_peak_indices['r_peaks'].append(r_peaks)\n",
    "\n",
    "        hr_sequence = np.full_like(ecg, fill_value=np.nan)\n",
    "        if len(r_peaks) > 1:\n",
    "            instant_hr = sampling_rate * 60.0 / np.diff(r_peaks)\n",
    "            interp_func = interp1d(r_peaks[1:], instant_hr, kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "            hr_sequence = interp_func(np.arange(len(ecg)))\n",
    "            hr_sequence = np.clip(hr_sequence, 40, 180)\n",
    "        all_hr_sequences.append(hr_sequence)\n",
    "\n",
    "        # --- PPG foot detection for PAT ---\n",
    "        ppg_feet, _ = find_peaks(-ppg, distance=sampling_rate * 60/80)\n",
    "        all_peak_indices['ppg_feet'].append(ppg_feet)\n",
    "\n",
    "        # --- PAT calculation ---\n",
    "        pat_sequence = np.full_like(ppg, fill_value=np.nan)\n",
    "        if len(r_peaks) > 0 and len(ppg_feet) > 0:\n",
    "            pat_values = []\n",
    "            corresponding_r_peaks = []\n",
    "            for r_peak in r_peaks:\n",
    "                following_feet = ppg_feet[ppg_feet > r_peak]\n",
    "                if len(following_feet) > 0:\n",
    "                    pat = (following_feet[0] - r_peak) / sampling_rate\n",
    "                    if 0.05 <= pat <= 0.5:\n",
    "                        pat_values.append(pat)\n",
    "                        corresponding_r_peaks.append(r_peak)\n",
    "            if len(pat_values) > 1:\n",
    "                interp_func = interp1d(corresponding_r_peaks, pat_values, kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "                pat_sequence = interp_func(np.arange(len(ppg)))\n",
    "                pat_sequence = np.clip(pat_sequence, 0.05, 0.5)\n",
    "        all_pat_sequences.append(pat_sequence)\n",
    "\n",
    "    pat_sequences = np.array(all_pat_sequences)\n",
    "    hr_sequences = np.array(all_hr_sequences)\n",
    "\n",
    "    # Fill remaining NaNs by forward/backward fill\n",
    "    pat_sequences = pd.DataFrame(pat_sequences).ffill(axis=1).bfill(axis=1).to_numpy()\n",
    "    hr_sequences = pd.DataFrame(hr_sequences).ffill(axis=1).bfill(axis=1).to_numpy()\n",
    "\n",
    "    print(f\"   - PAT sequences created with shape: {pat_sequences.shape}\")\n",
    "    print(f\"   - HR sequences created with shape: {hr_sequences.shape}\")\n",
    "    return pat_sequences, hr_sequences, all_peak_indices\n",
    "\n",
    "def standardize_feature_length(feature_sequences, target_length):\n",
    "    \"\"\"Standardize feature sequence length to match signal length.\"\"\"\n",
    "    standardized_features = []\n",
    "    for seq in feature_sequences:\n",
    "        if len(seq) > target_length:\n",
    "            start_idx = (len(seq) - target_length) // 2\n",
    "            standardized_features.append(seq[start_idx:start_idx + target_length])\n",
    "        else:\n",
    "            padding = target_length - len(seq)\n",
    "            pad_left = padding // 2\n",
    "            pad_right = padding - pad_left\n",
    "            standardized_features.append(np.pad(seq, (pad_left, pad_right), mode='edge'))\n",
    "    return np.array(standardized_features)\n",
    "\n",
    "\n",
    "if signals_agg is not None:\n",
    "    # Raw signals are (samples, channels, timesteps): channel 0 PPG, channel 1 ECG\n",
    "    ppg_raw = signals_agg[:, 0, :]\n",
    "    ecg_raw = signals_agg[:, 1, :]\n",
    "\n",
    "    pat_seqs, hr_seqs, peak_indices = extract_physiological_features(ecg_raw, ppg_raw)\n",
    "\n",
    "    # Standardize length to match processed_signals\n",
    "    target_len = processed_signals.shape[1]\n",
    "    pat_seqs = standardize_feature_length(pat_seqs, target_len)\n",
    "    hr_seqs = standardize_feature_length(hr_seqs, target_len)\n",
    "    print(f\"   - Physiological features standardized to length: {target_len}\")\n",
    "\n",
    "    # Scale features using training split only\n",
    "    print(\"\\nüîÑ Scaling physiological features (PAT, HR)...\")\n",
    "    scaler_pat = StandardScaler()\n",
    "    scaler_hr = StandardScaler()\n",
    "    scaler_pat.fit(pat_seqs[train_mask])\n",
    "    scaler_hr.fit(hr_seqs[train_mask])\n",
    "    pat_seqs_scaled = scaler_pat.transform(pat_seqs)\n",
    "    hr_seqs_scaled = scaler_hr.transform(hr_seqs)\n",
    "    print(\"   - Physiological features scaled.\")\n",
    "else:\n",
    "    pat_seqs = hr_seqs = peak_indices = None\n",
    "    pat_seqs_scaled = hr_seqs_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20054e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: BASELINE MODEL (LINEAR REGRESSION ON PHYSIOLOGICAL FEATURES)\n",
    "# =============================================================================\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "def create_baseline_features(pat_sequences, hr_sequences, labels):\n",
    "    \"\"\"\n",
    "    Creates a simple feature set for the baseline model.\n",
    "    Features: mean PAT, std PAT, mean HR, std HR.\n",
    "    \"\"\"\n",
    "    print(\"üõ†Ô∏è  Creating features for baseline model...\")\n",
    "    \n",
    "    # Check for NaN/Inf values in inputs\n",
    "    if np.any(np.isnan(pat_sequences)) or np.any(np.isinf(pat_sequences)):\n",
    "        print(\"   - Warning: NaN or Inf found in PAT sequences. Filling with 0.\")\n",
    "        pat_sequences = np.nan_to_num(pat_sequences)\n",
    "        \n",
    "    if np.any(np.isnan(hr_sequences)) or np.any(np.isinf(hr_sequences)):\n",
    "        print(\"   - Warning: NaN or Inf found in HR sequences. Filling with 0.\")\n",
    "        hr_sequences = np.nan_to_num(hr_sequences)\n",
    "\n",
    "    mean_pat = np.mean(pat_sequences, axis=1)\n",
    "    std_pat = np.std(pat_sequences, axis=1)\n",
    "    mean_hr = np.mean(hr_sequences, axis=1)\n",
    "    std_hr = np.std(hr_sequences, axis=1)\n",
    "    \n",
    "    X_baseline = np.column_stack([mean_pat, std_pat, mean_hr, std_hr])\n",
    "    y_baseline = labels\n",
    "    \n",
    "    print(f\"   - Baseline features created with shape: {X_baseline.shape}\")\n",
    "    return X_baseline, y_baseline\n",
    "\n",
    "# Create features\n",
    "X_baseline, y_baseline = create_baseline_features(pat_seqs, hr_seqs, y)\n",
    "\n",
    "if X_baseline.shape[0] > 10:\n",
    "    # Prefer subject-wise split (prevents patient leakage) if masks exist\n",
    "    have_subject_masks = (\n",
    "        'train_mask' in globals() and 'val_mask' in globals() and 'test_mask' in globals() and\n",
    "        train_mask is not None and val_mask is not None and test_mask is not None\n",
    "    )\n",
    "    \n",
    "    if have_subject_masks:\n",
    "        X_base_train, y_base_train = X_baseline[train_mask], y_baseline[train_mask]\n",
    "        X_base_val, y_base_val     = X_baseline[val_mask], y_baseline[val_mask]\n",
    "        X_base_test, y_base_test   = X_baseline[test_mask], y_baseline[test_mask]\n",
    "        split_name = \"subject-wise\"\n",
    "    else:\n",
    "        # Fallback: random split (less strict; can overestimate performance)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_base_train, X_base_test, y_base_train, y_base_test = train_test_split(\n",
    "            X_baseline, y_baseline, test_size=0.3, random_state=42\n",
    "        )\n",
    "        X_base_val, y_base_val = None, None\n",
    "        split_name = \"random\"\n",
    "\n",
    "    if X_base_train.shape[0] < 2 or X_base_test.shape[0] < 1:\n",
    "        print(\"\\n‚ö†Ô∏è  Not enough data after splitting to train/evaluate the baseline model.\")\n",
    "    else:\n",
    "        # Train model\n",
    "        print(f\"\\nü§ñ Training baseline Linear Regression model ({split_name} split)...\")\n",
    "        baseline_model = LinearRegression()\n",
    "        baseline_model.fit(X_base_train, y_base_train)\n",
    "        print(\"   - Baseline model trained.\")\n",
    "\n",
    "        # Evaluate model (test)\n",
    "        y_base_pred = baseline_model.predict(X_base_test)\n",
    "        mae = mean_absolute_error(y_base_test, y_base_pred)\n",
    "        r2 = r2_score(y_base_test, y_base_pred)\n",
    "\n",
    "        print(f\"\\n‚úÖ Baseline Model Evaluation (test):\")\n",
    "        print(f\"   - Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "        print(f\"   - R^2 Score: {r2:.2f}\")\n",
    "\n",
    "        # Optional: validation metrics when available\n",
    "        if X_base_val is not None and X_base_val.shape[0] > 0:\n",
    "            y_val_pred = baseline_model.predict(X_base_val)\n",
    "            mae_val = mean_absolute_error(y_base_val, y_val_pred)\n",
    "            r2_val = r2_score(y_base_val, y_val_pred)\n",
    "            print(f\"\\n‚úÖ Baseline Model Evaluation (val):\")\n",
    "            print(f\"   - Mean Absolute Error (MAE): {mae_val:.2f}\")\n",
    "            print(f\"   - R^2 Score: {r2_val:.2f}\")\n",
    "\n",
    "        # Plot results (test)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(y_base_test, y_base_pred, alpha=0.6, edgecolors='k')\n",
    "        plt.plot([min(y_base_test), max(y_base_test)], [min(y_base_test), max(y_base_test)], 'r--', lw=2, label='Ideal Fit')\n",
    "        plt.title('Baseline Model: Predicted vs. Actual SBP')\n",
    "        plt.xlabel('Actual SBP (mmHg)')\n",
    "        plt.ylabel('Predicted SBP (mmHg)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Not enough data to train and evaluate the baseline model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2767f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 4: PHYSIOLOGY-INFORMED DEEP LEARNING MODEL (IMPROVED)\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GRU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: NaN/Inf safety\n",
    "# -----------------------------\n",
    "def _ensure_finite(name, arr):\n",
    "    arr = np.asarray(arr)\n",
    "    finite_mask = np.isfinite(arr)\n",
    "    if finite_mask.all():\n",
    "        return arr\n",
    "    n_bad = arr.size - int(finite_mask.sum())\n",
    "    print(f\"‚ö†Ô∏è {name}: found {n_bad} non-finite values (NaN/Inf). Replacing with 0.\")\n",
    "    return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "def _ensure_finite_1d(name, y_arr):\n",
    "    \"\"\"Return (filtered_y, good_mask) where good_mask indexes the original y_arr.\"\"\"\n",
    "    y_arr = np.asarray(y_arr).reshape(-1)\n",
    "    finite_mask = np.isfinite(y_arr)\n",
    "    if finite_mask.all():\n",
    "        return y_arr, finite_mask\n",
    "    n_bad = y_arr.size - int(finite_mask.sum())\n",
    "    print(f\"‚ö†Ô∏è {name}: found {n_bad} non-finite labels. Dropping those samples.\")\n",
    "    return y_arr[finite_mask], finite_mask\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Create 4-channel input\n",
    "# -----------------------------\n",
    "def create_4_channel_input(raw_signals, pat_sequences, hr_sequences):\n",
    "    \"\"\"\n",
    "    Combines raw signals and physiological features into a 4-channel input.\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Creating 4-channel input for the physiology-informed model...\")\n",
    "\n",
    "    pat_expanded = np.expand_dims(pat_sequences, axis=-1)\n",
    "    hr_expanded = np.expand_dims(hr_sequences, axis=-1)\n",
    "\n",
    "    X_phys_informed = np.concatenate([raw_signals, pat_expanded, hr_expanded], axis=-1)\n",
    "    print(f\"   - 4-channel input created with shape: {X_phys_informed.shape}\")\n",
    "\n",
    "    return X_phys_informed\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Normalize Inputs (NaN-safe)\n",
    "# -----------------------------\n",
    "def normalize_data(X_train, X_val, X_test):\n",
    "    mean = np.nanmean(X_train, axis=0)\n",
    "    std = np.nanstd(X_train, axis=0) + 1e-8  # avoid division by zero\n",
    "    X_train_norm = (X_train - mean) / std\n",
    "    X_val_norm = (X_val - mean) / std\n",
    "    X_test_norm = (X_test - mean) / std\n",
    "    # If anything is still non-finite (e.g., all-NaN timesteps), zero it out\n",
    "    X_train_norm = np.nan_to_num(X_train_norm, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_val_norm = np.nan_to_num(X_val_norm, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_test_norm = np.nan_to_num(X_test_norm, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X_train_norm, X_val_norm, X_test_norm\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Construct Model\n",
    "# -----------------------------\n",
    "def create_phys_informed_model(input_shape):\n",
    "    \"\"\"\n",
    "    CNN + GRU model for 4-channel physiological input with BatchNorm and Dropout.\n",
    "    \"\"\"\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Conv1D(64, 5, activation=\"relu\", input_shape=input_shape, padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.3),\n",
    "            Conv1D(128, 5, activation=\"relu\", padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(2),\n",
    "            Dropout(0.3),\n",
    "            GRU(128, return_sequences=True),\n",
    "            Dropout(0.3),\n",
    "            GRU(64),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dense(1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    optimizer = Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss=Huber(), metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Prepare data\n",
    "# -----------------------------\n",
    "if signals_agg is not None:\n",
    "    # Create 4-channel input\n",
    "    X_phys_informed = create_4_channel_input(processed_signals, pat_seqs_scaled, hr_seqs_scaled)\n",
    "    X_phys_informed = _ensure_finite(\"X_phys_informed (pre-split)\", X_phys_informed)\n",
    "\n",
    "    # Apply masks\n",
    "    X_train_phys, y_train = X_phys_informed[train_mask], y[train_mask]\n",
    "    X_val_phys, y_val = X_phys_informed[val_mask], y[val_mask]\n",
    "    X_test_phys, y_test = X_phys_informed[test_mask], y[test_mask]\n",
    "\n",
    "    # Ensure labels are finite (drop invalid samples if any)\n",
    "    y_train, train_good = _ensure_finite_1d(\"y_train\", y_train)\n",
    "    y_val, val_good = _ensure_finite_1d(\"y_val\", y_val)\n",
    "    y_test, test_good = _ensure_finite_1d(\"y_test\", y_test)\n",
    "    X_train_phys = X_train_phys[train_good]\n",
    "    X_val_phys = X_val_phys[val_good]\n",
    "    X_test_phys = X_test_phys[test_good]\n",
    "\n",
    "    # Ensure inputs are finite\n",
    "    X_train_phys = _ensure_finite(\"X_train_phys (pre-norm)\", X_train_phys)\n",
    "    X_val_phys = _ensure_finite(\"X_val_phys (pre-norm)\", X_val_phys)\n",
    "    X_test_phys = _ensure_finite(\"X_test_phys (pre-norm)\", X_test_phys)\n",
    "\n",
    "    # Normalize\n",
    "    X_train_phys, X_val_phys, X_test_phys = normalize_data(X_train_phys, X_val_phys, X_test_phys)\n",
    "\n",
    "    print(f\"\\nüéØ Final data shapes:\")\n",
    "    print(f\"   X_train: {X_train_phys.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"   X_val:   {X_val_phys.shape}, y_val: {y_val.shape}\")\n",
    "    print(f\"   X_test:  {X_test_phys.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 5: Create and Train Model\n",
    "    # -----------------------------\n",
    "    if X_train_phys.shape[0] < 2 or X_val_phys.shape[0] < 1:\n",
    "        print(\"\\n‚ö†Ô∏è Not enough data to train/validate the physiology-informed model after filtering.\")\n",
    "    else:\n",
    "        phys_input_shape = (X_train_phys.shape[1], X_train_phys.shape[2])\n",
    "        phys_informed_model = create_phys_informed_model(phys_input_shape)\n",
    "        phys_informed_model.summary()\n",
    "\n",
    "        # Callbacks\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n",
    "\n",
    "        effective_batch_size = int(min(BATCH_SIZE, max(1, X_train_phys.shape[0])))\n",
    "        if effective_batch_size != BATCH_SIZE:\n",
    "            print(f\"\\n‚ÑπÔ∏è Adjusting batch_size from {BATCH_SIZE} to {effective_batch_size} (tiny dataset).\")\n",
    "\n",
    "        # Train\n",
    "        print(\"\\nüèÉ‚Äç‚ôÇÔ∏è Training Physiology-Informed Model...\")\n",
    "        phys_informed_history = phys_informed_model.fit(\n",
    "            X_train_phys,\n",
    "            y_train,\n",
    "            validation_data=(X_val_phys, y_val),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=effective_batch_size,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=VERBOSE,\n",
    "        )\n",
    "        print(\"   - Training complete.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No data to train the physiology-informed model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
