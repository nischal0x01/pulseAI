"""
Convert Keras H5 model to TensorFlow.js format
Workaround for Python 3.13 compatibility issues and Lambda layer issues
Compatible with Keras 3.x and TensorFlow.js
"""

import tensorflow as tf
from tensorflow import keras
import subprocess
import sys
import os

# Disable Keras 3 behavior to ensure compatibility with TFJS
os.environ['TF_USE_LEGACY_KERAS'] = '1'

# Add src directory to path to import model builder
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

print("="*60)
print("TFJS Model Converter (Keras 2.x compatibility mode)")
print("="*60)

# Rebuild model architecture and load weights
print("\n[1/4] Rebuilding model architecture and loading weights...")
try:
    # Import after setting environment variable
    from src.models.model_builder_attention import create_phys_informed_cnn_lstm_attention
    
    # Create model architecture with Lambda layer that has output_shape
    print("  - Creating model architecture...")
    model = create_phys_informed_cnn_lstm_attention(input_shape=(875, 4))
    
    # Load weights from H5 file
    print("  - Loading weights from checkpoints/best_model.h5...")
    model.load_weights('checkpoints/best_model.h5')
    
    print(f"✓ Model loaded successfully")
    print(f"  - Input shape: {model.input_shape}")
    print(f"  - Output shapes: {[out.shape for out in model.outputs]}")
except Exception as e:
    print(f"✗ Failed to load model: {e}")
    print("\nTrying to load model with compile=False...")
    try:
        # Try direct load with compile=False as last resort
        model = tf.keras.models.load_model(
            'checkpoints/best_model.h5',
            compile=False,
            safe_mode=False
        )
        print(f"✓ Model loaded successfully (without compilation)")
        print(f"  - Input shape: {model.input_shape}")
        print(f"  - Output shape: {model.output_shape}")
    except Exception as e2:
        print(f"✗ All approaches failed: {e2}")
        sys.exit(1)

# Save in SavedModel format for better compatibility
saved_model_path = 'temp_saved_model'
print(f"\n[2/4] Saving as SavedModel format to {saved_model_path}...")
try:
    # Remove old saved model if exists
    import shutil
    if os.path.exists(saved_model_path):
        shutil.rmtree(saved_model_path)
    
    # Save in SavedModel format (better compatibility)
    model.save(saved_model_path, save_format='tf')
    print(f"✓ Saved successfully")
except Exception as e:
    print(f"✗ Failed to save: {e}")
    # Fallback to .keras format
    keras_path = 'temp_model.keras'
    print(f"\nFalling back to Keras format: {keras_path}...")
    try:
        model.save(keras_path)
        saved_model_path = keras_path
        print(f"✓ Saved successfully")
    except Exception as e2:
        print(f"✗ Failed to save: {e2}")
        sys.exit(1)

# Create a test to verify model works
print(f"\n[3/4] Testing model inference...")
try:
    import numpy as np
    test_input = np.random.randn(1, 875, 4).astype(np.float32)
    test_output = model.predict(test_input, verbose=0)
    print(f"✓ Model inference works")
    print(f"  - Test output shapes: {[out.shape for out in test_output]}")
except Exception as e:
    print(f"⚠ Warning: Model inference test failed: {e}")
    print("  - Continuing anyway...")

# Try to convert using tensorflowjs_converter
output_path = 'frontend/public/models'
print(f"\n[4/4] Converting to TensorFlow.js format...")
print(f"  Output: {output_path}")

# Determine input format based on what we saved
if os.path.isdir(saved_model_path):
    input_format = 'tf_saved_model'
else:
    input_format = 'keras'

cmd = [
    'tensorflowjs_converter',
    f'--input_format={input_format}',
    '--skip_op_check',  # Skip unsupported op checks
    '--strip_debug_ops=True',  # Remove debug operations
    saved_model_path,
    output_path
]

print(f"  Command: {' '.join(cmd)}")

try:
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode == 0:
        print("✓ Conversion successful!")
        print(f"\nYour model is ready at: {output_path}")
        print("\nUsage in Next.js:")
        print("  const model = await tf.loadLayersModel('/models/model.json');")
        
        # Verify the output
        model_json_path = os.path.join(output_path, 'model.json')
        if os.path.exists(model_json_path):
            import json
            with open(model_json_path, 'r') as f:
                model_json = json.load(f)
            print(f"\n✓ Verified model.json exists")
            print(f"  Format: {model_json.get('format', 'unknown')}")
            print(f"  Generated by: {model_json.get('generatedBy', 'unknown')}")
        
    else:
        print("✗ Conversion failed!")
        print("\nSTDOUT:")
        print(result.stdout)
        print("\nSTDERR:")
        print(result.stderr)
        print("\n" + "="*60)
        print("TROUBLESHOOTING:")
        print("="*60)
        print("\n1. Check tensorflowjs version:")
        print("   pip show tensorflowjs")
        print("   Recommended: tensorflowjs>=3.18.0")
        print("\n2. Try with different Python version (3.10 recommended):")
        print("   conda create -n tfjs python=3.10")
        print("   conda activate tfjs")
        print("   pip install tensorflowjs tensorflow")
        print(f"   python src/tfjs_converter.py")
        print("\n3. If Lambda layers cause issues, they need custom_objects:")
        print("   Check the extract_pat Lambda layer in your model")
        sys.exit(1)
        
except FileNotFoundError:
    print("✗ tensorflowjs_converter not found!")
    print("\nInstall it with: pip install tensorflowjs")
    print("Note: Requires Python 3.8-3.11 (Python 3.13 not supported)")
    sys.exit(1)

# Cleanup
print(f"\n[5/5] Cleaning up temporary files...")
try:
    import shutil
    if os.path.exists('temp_saved_model'):
        shutil.rmtree('temp_saved_model')
    if os.path.exists('temp_model.keras'):
        os.remove('temp_model.keras')
    print("✓ Cleanup complete")
except Exception as e:
    print(f"⚠ Warning: Cleanup failed: {e}")